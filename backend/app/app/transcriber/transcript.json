{"task": "transcribe", "language": "english", "duration": 524.07, "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.84, "text": " Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise", "tokens": [2421, 11, 286, 478, 510, 281, 5366, 281, 291, 11, 291, 366, 527, 700, 490, 1379, 46731, 320, 1605, 295, 26696], "temperature": 0.0, "avg_logprob": -0.34817066899052374, "compression_ratio": 1.4768518518518519, "no_speech_prob": 0.3598945140838623, "transient": false}, {"id": 1, "seek": 0, "start": 8.84, "end": 9.84, "text": " Assistants.", "tokens": [49633, 1719, 13], "temperature": 0.0, "avg_logprob": -0.34817066899052374, "compression_ratio": 1.4768518518518519, "no_speech_prob": 0.3598945140838623, "transient": false}, {"id": 2, "seek": 0, "start": 9.84, "end": 16.56, "text": " Juan is responsible for knowing everything in your company, but first things first, our", "tokens": [17064, 307, 6250, 337, 5276, 1203, 294, 428, 2237, 11, 457, 700, 721, 700, 11, 527], "temperature": 0.0, "avg_logprob": -0.34817066899052374, "compression_ratio": 1.4768518518518519, "no_speech_prob": 0.3598945140838623, "transient": false}, {"id": 3, "seek": 0, "start": 16.56, "end": 23.400000000000002, "text": " team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather", "tokens": [1469, 307, 490, 31571, 11, 376, 2641, 27924, 11, 26501, 11, 293, 286, 5665, 1621, 294, 7244, 11, 293, 321, 439, 5448], "temperature": 0.0, "avg_logprob": -0.34817066899052374, "compression_ratio": 1.4768518518518519, "no_speech_prob": 0.3598945140838623, "transient": false}, {"id": 4, "seek": 0, "start": 23.400000000000002, "end": 26.0, "text": " together to solve one simple problem.", "tokens": [1214, 281, 5039, 472, 2199, 1154, 13], "temperature": 0.0, "avg_logprob": -0.34817066899052374, "compression_ratio": 1.4768518518518519, "no_speech_prob": 0.3598945140838623, "transient": false}, {"id": 5, "seek": 2600, "start": 26.0, "end": 30.92, "text": " The problem is that we are really spoiled by quality of Google search.", "tokens": [440, 1154, 307, 300, 321, 366, 534, 32439, 538, 3125, 295, 3329, 3164, 13], "temperature": 0.0, "avg_logprob": -0.38286988453198506, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.271639282815158e-05, "transient": false}, {"id": 6, "seek": 2600, "start": 30.92, "end": 37.68, "text": " So spoiled that every time we need to find something in our corporate systems, we suffer,", "tokens": [407, 32439, 300, 633, 565, 321, 643, 281, 915, 746, 294, 527, 10896, 3652, 11, 321, 9753, 11], "temperature": 0.0, "avg_logprob": -0.38286988453198506, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.271639282815158e-05, "transient": false}, {"id": 7, "seek": 2600, "start": 37.68, "end": 43.28, "text": " because they usually use the keyword search, the old style, not the modern one that Google,", "tokens": [570, 436, 2673, 764, 264, 20428, 3164, 11, 264, 1331, 3758, 11, 406, 264, 4363, 472, 300, 3329, 11], "temperature": 0.0, "avg_logprob": -0.38286988453198506, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.271639282815158e-05, "transient": false}, {"id": 8, "seek": 2600, "start": 43.28, "end": 47.86, "text": " not modern one, semantic, that Google use.", "tokens": [406, 4363, 472, 11, 47982, 11, 300, 3329, 764, 13], "temperature": 0.0, "avg_logprob": -0.38286988453198506, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.271639282815158e-05, "transient": false}, {"id": 9, "seek": 2600, "start": 47.86, "end": 55.44, "text": " But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast,", "tokens": [583, 264, 1154, 307, 709, 3801, 11, 300, 257, 688, 295, 3601, 307, 10596, 294, 264, 8287, 3670, 3734, 11], "temperature": 0.0, "avg_logprob": -0.38286988453198506, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.271639282815158e-05, "transient": false}, {"id": 10, "seek": 5544, "start": 55.44, "end": 58.04, "text": " like you are looking now.", "tokens": [411, 291, 366, 1237, 586, 13], "temperature": 0.0, "avg_logprob": -0.40586188260246725, "compression_ratio": 1.4568527918781726, "no_speech_prob": 6.450530781876296e-05, "transient": false}, {"id": 11, "seek": 5544, "start": 58.04, "end": 65.32, "text": " Imagine that all video presentation of business processes, systems, solutions, or ever greater", "tokens": [11739, 300, 439, 960, 5860, 295, 1606, 7555, 11, 3652, 11, 6547, 11, 420, 1562, 5044], "temperature": 0.0, "avg_logprob": -0.40586188260246725, "compression_ratio": 1.4568527918781726, "no_speech_prob": 6.450530781876296e-05, "transient": false}, {"id": 12, "seek": 5544, "start": 65.32, "end": 72.47999999999999, "text": " ideas in your company will be converted into the Wikipedia pages and will be searchable", "tokens": [3487, 294, 428, 2237, 486, 312, 16424, 666, 264, 28999, 7183, 293, 486, 312, 3164, 712], "temperature": 0.0, "avg_logprob": -0.40586188260246725, "compression_ratio": 1.4568527918781726, "no_speech_prob": 6.450530781876296e-05, "transient": false}, {"id": 13, "seek": 5544, "start": 72.47999999999999, "end": 75.72, "text": " by modern search engines.", "tokens": [538, 4363, 3164, 12982, 13], "temperature": 0.0, "avg_logprob": -0.40586188260246725, "compression_ratio": 1.4568527918781726, "no_speech_prob": 6.450530781876296e-05, "transient": false}, {"id": 14, "seek": 5544, "start": 75.72, "end": 80.47999999999999, "text": " And that's exactly our, what Juan Holloway is doing.", "tokens": [400, 300, 311, 2293, 527, 11, 437, 17064, 46731, 320, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.40586188260246725, "compression_ratio": 1.4568527918781726, "no_speech_prob": 6.450530781876296e-05, "transient": false}, {"id": 15, "seek": 8048, "start": 80.48, "end": 85.76, "text": " Let me show you.", "tokens": [961, 385, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.2952813662015475, "compression_ratio": 1.4941176470588236, "no_speech_prob": 2.4228347683674656e-06, "transient": false}, {"id": 16, "seek": 8048, "start": 85.76, "end": 94.46000000000001, "text": " To make a demo, we convert some results from previous hackathons into our Wikipedia.", "tokens": [1407, 652, 257, 10723, 11, 321, 7620, 512, 3542, 490, 3894, 10339, 998, 892, 666, 527, 28999, 13], "temperature": 0.0, "avg_logprob": -0.2952813662015475, "compression_ratio": 1.4941176470588236, "no_speech_prob": 2.4228347683674656e-06, "transient": false}, {"id": 17, "seek": 8048, "start": 94.46000000000001, "end": 102.36, "text": " So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some", "tokens": [407, 309, 311, 13735, 28999, 11, 28999, 300, 2089, 291, 8129, 7183, 11, 909, 777, 2306, 11, 362, 512], "temperature": 0.0, "avg_logprob": -0.2952813662015475, "compression_ratio": 1.4941176470588236, "no_speech_prob": 2.4228347683674656e-06, "transient": false}, {"id": 18, "seek": 8048, "start": 102.36, "end": 108.60000000000001, "text": " structure here, and whatever, just editor for the small pages.", "tokens": [3877, 510, 11, 293, 2035, 11, 445, 9839, 337, 264, 1359, 7183, 13], "temperature": 0.0, "avg_logprob": -0.2952813662015475, "compression_ratio": 1.4941176470588236, "no_speech_prob": 2.4228347683674656e-06, "transient": false}, {"id": 19, "seek": 10860, "start": 108.6, "end": 117.67999999999999, "text": " So we converted demos of a few teams into this Wikipedia format using Whisper and some", "tokens": [407, 321, 16424, 33788, 295, 257, 1326, 5491, 666, 341, 28999, 7877, 1228, 41132, 610, 293, 512], "temperature": 0.0, "avg_logprob": -0.24560068731438622, "compression_ratio": 1.513089005235602, "no_speech_prob": 6.537604804179864e-06, "transient": false}, {"id": 20, "seek": 10860, "start": 117.67999999999999, "end": 121.67999999999999, "text": " magic from GPT to make them much more readable.", "tokens": [5585, 490, 26039, 51, 281, 652, 552, 709, 544, 49857, 13], "temperature": 0.0, "avg_logprob": -0.24560068731438622, "compression_ratio": 1.513089005235602, "no_speech_prob": 6.537604804179864e-06, "transient": false}, {"id": 21, "seek": 10860, "start": 121.67999999999999, "end": 129.07999999999998, "text": " But what is more important, we allow them to be searchable by semantic search.", "tokens": [583, 437, 307, 544, 1021, 11, 321, 2089, 552, 281, 312, 3164, 712, 538, 47982, 3164, 13], "temperature": 0.0, "avg_logprob": -0.24560068731438622, "compression_ratio": 1.513089005235602, "no_speech_prob": 6.537604804179864e-06, "transient": false}, {"id": 22, "seek": 10860, "start": 129.07999999999998, "end": 134.32, "text": " So we use a multilingual semantic search, and I will show you how it works.", "tokens": [407, 321, 764, 257, 2120, 38219, 47982, 3164, 11, 293, 286, 486, 855, 291, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.24560068731438622, "compression_ratio": 1.513089005235602, "no_speech_prob": 6.537604804179864e-06, "transient": false}, {"id": 23, "seek": 13432, "start": 134.32, "end": 141.2, "text": " We have some project, Ask Quran, let's try to formulate it in Russian, like a questions", "tokens": [492, 362, 512, 1716, 11, 12320, 19375, 11, 718, 311, 853, 281, 47881, 309, 294, 7220, 11, 411, 257, 1651], "temperature": 0.0, "avg_logprob": -0.560177729679988, "compression_ratio": 1.2242990654205608, "no_speech_prob": 9.144811338046566e-05, "transient": false}, {"id": 24, "seek": 13432, "start": 141.2, "end": 144.92, "text": " about God.", "tokens": [466, 1265, 13], "temperature": 0.0, "avg_logprob": -0.560177729679988, "compression_ratio": 1.2242990654205608, "no_speech_prob": 9.144811338046566e-05, "transient": false}, {"id": 25, "seek": 13432, "start": 144.92, "end": 155.92, "text": " Yeah, we have Ask Quran project.", "tokens": [865, 11, 321, 362, 12320, 19375, 1716, 13], "temperature": 0.0, "avg_logprob": -0.560177729679988, "compression_ratio": 1.2242990654205608, "no_speech_prob": 9.144811338046566e-05, "transient": false}, {"id": 26, "seek": 15592, "start": 155.92, "end": 164.92, "text": " It's more delay on loading.", "tokens": [467, 311, 544, 8577, 322, 15114, 13], "temperature": 0.0, "avg_logprob": -0.544442892074585, "compression_ratio": 1.2300884955752212, "no_speech_prob": 1.7054564978025155e-06, "transient": false}, {"id": 27, "seek": 15592, "start": 164.92, "end": 178.67999999999998, "text": " I will skip a bit.", "tokens": [286, 486, 10023, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.544442892074585, "compression_ratio": 1.2300884955752212, "no_speech_prob": 1.7054564978025155e-06, "transient": false}, {"id": 28, "seek": 15592, "start": 178.67999999999998, "end": 185.35999999999999, "text": " Seems like it's quite a good match about questions about God in Russian to the English text.", "tokens": [22524, 411, 309, 311, 1596, 257, 665, 2995, 466, 1651, 466, 1265, 294, 7220, 281, 264, 3669, 2487, 13], "temperature": 0.0, "avg_logprob": -0.544442892074585, "compression_ratio": 1.2300884955752212, "no_speech_prob": 1.7054564978025155e-06, "transient": false}, {"id": 29, "seek": 18536, "start": 185.36, "end": 193.70000000000002, "text": " So let me show you how our system, our Wikipedias works with video.", "tokens": [407, 718, 385, 855, 291, 577, 527, 1185, 11, 527, 23377, 647, 292, 4609, 1985, 365, 960, 13], "temperature": 0.0, "avg_logprob": -0.35132277453387223, "compression_ratio": 1.5245901639344261, "no_speech_prob": 3.5311561077833176e-05, "transient": false}, {"id": 30, "seek": 18536, "start": 193.70000000000002, "end": 205.64000000000001, "text": " So when video is playing, we highlight the part of the video, or you can jump to the", "tokens": [407, 562, 960, 307, 2433, 11, 321, 5078, 264, 644, 295, 264, 960, 11, 420, 291, 393, 3012, 281, 264], "temperature": 0.0, "avg_logprob": -0.35132277453387223, "compression_ratio": 1.5245901639344261, "no_speech_prob": 3.5311561077833176e-05, "transient": false}, {"id": 31, "seek": 18536, "start": 205.64000000000001, "end": 212.72000000000003, "text": " part of the video from the video.", "tokens": [644, 295, 264, 960, 490, 264, 960, 13], "temperature": 0.0, "avg_logprob": -0.35132277453387223, "compression_ratio": 1.5245901639344261, "no_speech_prob": 3.5311561077833176e-05, "transient": false}, {"id": 32, "seek": 21272, "start": 212.72, "end": 219.68, "text": " We could jump to the timestamp in the video, clicking to the text, or you can switch the", "tokens": [492, 727, 3012, 281, 264, 49108, 1215, 294, 264, 960, 11, 9697, 281, 264, 2487, 11, 420, 291, 393, 3679, 264], "temperature": 0.0, "avg_logprob": -0.28872651639192, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269177977927029e-05, "transient": false}, {"id": 33, "seek": 21272, "start": 219.68, "end": 223.28, "text": " video part and see which part of the text it is.", "tokens": [960, 644, 293, 536, 597, 644, 295, 264, 2487, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.28872651639192, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269177977927029e-05, "transient": false}, {"id": 34, "seek": 21272, "start": 223.28, "end": 231.36, "text": " Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search.", "tokens": [876, 11, 309, 311, 257, 857, 484, 295, 11923, 295, 2190, 10339, 18660, 11, 293, 321, 366, 1417, 466, 47982, 3164, 13], "temperature": 0.0, "avg_logprob": -0.28872651639192, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269177977927029e-05, "transient": false}, {"id": 35, "seek": 21272, "start": 231.36, "end": 238.88, "text": " So I will try to write in Spanish, not as personales, which is personal nouns, and see", "tokens": [407, 286, 486, 853, 281, 2464, 294, 8058, 11, 406, 382, 2973, 279, 11, 597, 307, 2973, 48184, 11, 293, 536], "temperature": 0.0, "avg_logprob": -0.28872651639192, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269177977927029e-05, "transient": false}, {"id": 36, "seek": 21272, "start": 238.88, "end": 239.88, "text": " what we get.", "tokens": [437, 321, 483, 13], "temperature": 0.0, "avg_logprob": -0.28872651639192, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269177977927029e-05, "transient": false}, {"id": 37, "seek": 23988, "start": 239.88, "end": 247.79999999999998, "text": " So we have a daily journaling app presentation.", "tokens": [407, 321, 362, 257, 5212, 17598, 4270, 724, 5860, 13], "temperature": 0.0, "avg_logprob": -0.3481828717217929, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.3499182387022302e-05, "transient": false}, {"id": 38, "seek": 23988, "start": 247.79999999999998, "end": 251.07999999999998, "text": " To speed up, I will click here.", "tokens": [1407, 3073, 493, 11, 286, 486, 2052, 510, 13], "temperature": 0.0, "avg_logprob": -0.3481828717217929, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.3499182387022302e-05, "transient": false}, {"id": 39, "seek": 23988, "start": 251.07999999999998, "end": 258.32, "text": " In conversational daily journaling experience, where we hope to enable users to write your", "tokens": [682, 2615, 1478, 5212, 17598, 4270, 1752, 11, 689, 321, 1454, 281, 9528, 5022, 281, 2464, 428], "temperature": 0.0, "avg_logprob": -0.3481828717217929, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.3499182387022302e-05, "transient": false}, {"id": 40, "seek": 23988, "start": 258.32, "end": 263.36, "text": " way to a better day.", "tokens": [636, 281, 257, 1101, 786, 13], "temperature": 0.0, "avg_logprob": -0.3481828717217929, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.3499182387022302e-05, "transient": false}, {"id": 41, "seek": 23988, "start": 263.36, "end": 267.88, "text": " That seems quite a good match for the questions, not as personales.", "tokens": [663, 2544, 1596, 257, 665, 2995, 337, 264, 1651, 11, 406, 382, 2973, 279, 13], "temperature": 0.0, "avg_logprob": -0.3481828717217929, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.3499182387022302e-05, "transient": false}, {"id": 42, "seek": 26788, "start": 267.88, "end": 273.71999999999997, "text": " Well, enough for the glamour, back to slides.", "tokens": [1042, 11, 1547, 337, 264, 28133, 396, 11, 646, 281, 9788, 13], "temperature": 0.0, "avg_logprob": -0.33751848581674937, "compression_ratio": 1.4682926829268292, "no_speech_prob": 3.6826440918957815e-05, "transient": false}, {"id": 43, "seek": 26788, "start": 273.71999999999997, "end": 280.0, "text": " Before jumping into the technical details, I would like to say that this knowledge management", "tokens": [4546, 11233, 666, 264, 6191, 4365, 11, 286, 576, 411, 281, 584, 300, 341, 3601, 4592], "temperature": 0.0, "avg_logprob": -0.33751848581674937, "compression_ratio": 1.4682926829268292, "no_speech_prob": 3.6826440918957815e-05, "transient": false}, {"id": 44, "seek": 26788, "start": 280.0, "end": 282.28, "text": " market is huge.", "tokens": [2142, 307, 2603, 13], "temperature": 0.0, "avg_logprob": -0.33751848581674937, "compression_ratio": 1.4682926829268292, "no_speech_prob": 3.6826440918957815e-05, "transient": false}, {"id": 45, "seek": 26788, "start": 282.28, "end": 291.44, "text": " It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two", "tokens": [467, 311, 12690, 294, 6779, 295, 17375, 295, 3808, 257, 1064, 11, 293, 309, 307, 4194, 1596, 2370, 337, 732], "temperature": 0.0, "avg_logprob": -0.33751848581674937, "compression_ratio": 1.4682926829268292, "no_speech_prob": 3.6826440918957815e-05, "transient": false}, {"id": 46, "seek": 26788, "start": 291.44, "end": 296.12, "text": " digits figures, depending on which report you read.", "tokens": [27011, 9624, 11, 5413, 322, 597, 2275, 291, 1401, 13], "temperature": 0.0, "avg_logprob": -0.33751848581674937, "compression_ratio": 1.4682926829268292, "no_speech_prob": 3.6826440918957815e-05, "transient": false}, {"id": 47, "seek": 29612, "start": 296.12, "end": 306.32, "text": " And it's quite a good trend to disrupt this market by new AI solutions or AI technologies,", "tokens": [400, 309, 311, 1596, 257, 665, 6028, 281, 14124, 341, 2142, 538, 777, 7318, 6547, 420, 7318, 7943, 11], "temperature": 0.0, "avg_logprob": -0.3493703206380208, "compression_ratio": 1.446236559139785, "no_speech_prob": 4.980812445865013e-05, "transient": false}, {"id": 48, "seek": 29612, "start": 306.32, "end": 308.52, "text": " like our IHANA.", "tokens": [411, 527, 286, 39, 21429, 13], "temperature": 0.0, "avg_logprob": -0.3493703206380208, "compression_ratio": 1.446236559139785, "no_speech_prob": 4.980812445865013e-05, "transient": false}, {"id": 49, "seek": 29612, "start": 308.52, "end": 315.68, "text": " So that could not be just a fancy toy with technology, but it could be a part of the", "tokens": [407, 300, 727, 406, 312, 445, 257, 10247, 12058, 365, 2899, 11, 457, 309, 727, 312, 257, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.3493703206380208, "compression_ratio": 1.446236559139785, "no_speech_prob": 4.980812445865013e-05, "transient": false}, {"id": 50, "seek": 29612, "start": 315.68, "end": 322.0, "text": " business because this solution is applicable in every enterprise environment.", "tokens": [1606, 570, 341, 3827, 307, 21142, 294, 633, 14132, 2823, 13], "temperature": 0.0, "avg_logprob": -0.3493703206380208, "compression_ratio": 1.446236559139785, "no_speech_prob": 4.980812445865013e-05, "transient": false}, {"id": 51, "seek": 32200, "start": 322.0, "end": 331.72, "text": " Well, then it could help the people who cannot listen to or watch videos to read information.", "tokens": [1042, 11, 550, 309, 727, 854, 264, 561, 567, 2644, 2140, 281, 420, 1159, 2145, 281, 1401, 1589, 13], "temperature": 0.0, "avg_logprob": -0.3417972067128057, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.900524749653414e-05, "transient": false}, {"id": 52, "seek": 32200, "start": 331.72, "end": 340.4, "text": " And I would say you can help busy managers to spend some time reading instead of watching", "tokens": [400, 286, 576, 584, 291, 393, 854, 5856, 14084, 281, 3496, 512, 565, 3760, 2602, 295, 1976], "temperature": 0.0, "avg_logprob": -0.3417972067128057, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.900524749653414e-05, "transient": false}, {"id": 53, "seek": 32200, "start": 340.4, "end": 341.8, "text": " videos.", "tokens": [2145, 13], "temperature": 0.0, "avg_logprob": -0.3417972067128057, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.900524749653414e-05, "transient": false}, {"id": 54, "seek": 32200, "start": 341.8, "end": 349.32, "text": " Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the", "tokens": [1743, 510, 11, 321, 393, 3079, 294, 2177, 455, 44990, 7318, 570, 321, 362, 257, 688, 295, 3601, 8287, 281, 264], "temperature": 0.0, "avg_logprob": -0.3417972067128057, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.900524749653414e-05, "transient": false}, {"id": 55, "seek": 34932, "start": 349.32, "end": 355.56, "text": " webcasts, but they are not searchable, so I guess it's not accessible.", "tokens": [3670, 3734, 82, 11, 457, 436, 366, 406, 3164, 712, 11, 370, 286, 2041, 309, 311, 406, 9515, 13], "temperature": 0.0, "avg_logprob": -0.36406986530010516, "compression_ratio": 1.349112426035503, "no_speech_prob": 3.717320942087099e-05, "transient": false}, {"id": 56, "seek": 34932, "start": 355.56, "end": 360.04, "text": " Well, how it was done.", "tokens": [1042, 11, 577, 309, 390, 1096, 13], "temperature": 0.0, "avg_logprob": -0.36406986530010516, "compression_ratio": 1.349112426035503, "no_speech_prob": 3.717320942087099e-05, "transient": false}, {"id": 57, "seek": 34932, "start": 360.04, "end": 363.44, "text": " As a base, we use an open source knowledge management system, Wikijs.", "tokens": [1018, 257, 3096, 11, 321, 764, 364, 1269, 4009, 3601, 4592, 1185, 11, 23377, 1718, 82, 13], "temperature": 0.0, "avg_logprob": -0.36406986530010516, "compression_ratio": 1.349112426035503, "no_speech_prob": 3.717320942087099e-05, "transient": false}, {"id": 58, "seek": 34932, "start": 363.44, "end": 370.03999999999996, "text": " It's quite commercially used and popular, written in JavaScript.", "tokens": [467, 311, 1596, 41751, 1143, 293, 3743, 11, 3720, 294, 15778, 13], "temperature": 0.0, "avg_logprob": -0.36406986530010516, "compression_ratio": 1.349112426035503, "no_speech_prob": 3.717320942087099e-05, "transient": false}, {"id": 59, "seek": 37004, "start": 370.04, "end": 379.52000000000004, "text": " It already supports some amount of search engines that you can set up and it will use", "tokens": [467, 1217, 9346, 512, 2372, 295, 3164, 12982, 300, 291, 393, 992, 493, 293, 309, 486, 764], "temperature": 0.0, "avg_logprob": -0.397910246977935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.8154952815384604e-05, "transient": false}, {"id": 60, "seek": 37004, "start": 379.52000000000004, "end": 380.64000000000004, "text": " as a search engine.", "tokens": [382, 257, 3164, 2848, 13], "temperature": 0.0, "avg_logprob": -0.397910246977935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.8154952815384604e-05, "transient": false}, {"id": 61, "seek": 37004, "start": 380.64000000000004, "end": 388.04, "text": " So we forked it and implement our OWASP, a basic combination of coherent and quadrant", "tokens": [407, 321, 17716, 292, 309, 293, 4445, 527, 38329, 3160, 47, 11, 257, 3875, 6562, 295, 36239, 293, 46856], "temperature": 0.0, "avg_logprob": -0.397910246977935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.8154952815384604e-05, "transient": false}, {"id": 62, "seek": 37004, "start": 388.04, "end": 390.76, "text": " services.", "tokens": [3328, 13], "temperature": 0.0, "avg_logprob": -0.397910246977935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.8154952815384604e-05, "transient": false}, {"id": 63, "seek": 37004, "start": 390.76, "end": 399.96000000000004, "text": " So now our fork support set up all these coherent and quadrant search systems with EPI case,", "tokens": [407, 586, 527, 17716, 1406, 992, 493, 439, 613, 36239, 293, 46856, 3164, 3652, 365, 25330, 40, 1389, 11], "temperature": 0.0, "avg_logprob": -0.397910246977935, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.8154952815384604e-05, "transient": false}, {"id": 64, "seek": 39996, "start": 399.96, "end": 405.12, "text": " and that's exactly how our demo worked.", "tokens": [293, 300, 311, 2293, 577, 527, 10723, 2732, 13], "temperature": 0.0, "avg_logprob": -0.3620251989983893, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.578921289066784e-05, "transient": false}, {"id": 65, "seek": 39996, "start": 405.12, "end": 411.15999999999997, "text": " Internally from the technical perspective, we convert each paragraph of the text on each", "tokens": [4844, 379, 490, 264, 6191, 4585, 11, 321, 7620, 1184, 18865, 295, 264, 2487, 322, 1184], "temperature": 0.0, "avg_logprob": -0.3620251989983893, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.578921289066784e-05, "transient": false}, {"id": 66, "seek": 39996, "start": 411.15999999999997, "end": 416.2, "text": " page into the vector using coherent embedding multilingual model.", "tokens": [3028, 666, 264, 8062, 1228, 36239, 12240, 3584, 2120, 38219, 2316, 13], "temperature": 0.0, "avg_logprob": -0.3620251989983893, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.578921289066784e-05, "transient": false}, {"id": 67, "seek": 39996, "start": 416.2, "end": 422.96, "text": " Then we just index and store them in the vector database provided by quadrant.", "tokens": [1396, 321, 445, 8186, 293, 3531, 552, 294, 264, 8062, 8149, 5649, 538, 46856, 13], "temperature": 0.0, "avg_logprob": -0.3620251989983893, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.578921289066784e-05, "transient": false}, {"id": 68, "seek": 39996, "start": 422.96, "end": 426.84, "text": " And when we have a query, we do almost the same.", "tokens": [400, 562, 321, 362, 257, 14581, 11, 321, 360, 1920, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.3620251989983893, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.578921289066784e-05, "transient": false}, {"id": 69, "seek": 42684, "start": 426.84, "end": 433.03999999999996, "text": " We convert it to the embedding and after that we search to the nearest embeddings in the", "tokens": [492, 7620, 309, 281, 264, 12240, 3584, 293, 934, 300, 321, 3164, 281, 264, 23831, 12240, 29432, 294, 264], "temperature": 0.0, "avg_logprob": -0.3008803515367105, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.872039840440266e-05, "transient": false}, {"id": 70, "seek": 42684, "start": 433.03999999999996, "end": 435.2, "text": " quadrant vector database.", "tokens": [46856, 8062, 8149, 13], "temperature": 0.0, "avg_logprob": -0.3008803515367105, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.872039840440266e-05, "transient": false}, {"id": 71, "seek": 42684, "start": 435.2, "end": 443.71999999999997, "text": " And magically, our link text to these vectors are quite close to the answers to the questions", "tokens": [400, 39763, 11, 527, 2113, 2487, 281, 613, 18875, 366, 1596, 1998, 281, 264, 6338, 281, 264, 1651], "temperature": 0.0, "avg_logprob": -0.3008803515367105, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.872039840440266e-05, "transient": false}, {"id": 72, "seek": 42684, "start": 443.71999999999997, "end": 446.52, "text": " we put into the query.", "tokens": [321, 829, 666, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.3008803515367105, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.872039840440266e-05, "transient": false}, {"id": 73, "seek": 42684, "start": 446.52, "end": 450.14, "text": " That's a kind of the AI magic, I guess.", "tokens": [663, 311, 257, 733, 295, 264, 7318, 5585, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.3008803515367105, "compression_ratio": 1.5847953216374269, "no_speech_prob": 2.872039840440266e-05, "transient": false}, {"id": 74, "seek": 45014, "start": 450.14, "end": 459.12, "text": " So the future work, first of all, current implementation is super naive of search system.", "tokens": [407, 264, 2027, 589, 11, 700, 295, 439, 11, 2190, 11420, 307, 1687, 29052, 295, 3164, 1185, 13], "temperature": 0.0, "avg_logprob": -0.32167283419905035, "compression_ratio": 1.4404761904761905, "no_speech_prob": 4.835758591070771e-05, "transient": false}, {"id": 75, "seek": 45014, "start": 459.12, "end": 463.76, "text": " Maybe we should use a bit more complex algorithm.", "tokens": [2704, 321, 820, 764, 257, 857, 544, 3997, 9284, 13], "temperature": 0.0, "avg_logprob": -0.32167283419905035, "compression_ratio": 1.4404761904761905, "no_speech_prob": 4.835758591070771e-05, "transient": false}, {"id": 76, "seek": 45014, "start": 463.76, "end": 473.71999999999997, "text": " But the funny, it takes like 300 lines of code to implement this magically high quality", "tokens": [583, 264, 4074, 11, 309, 2516, 411, 6641, 3876, 295, 3089, 281, 4445, 341, 39763, 1090, 3125], "temperature": 0.0, "avg_logprob": -0.32167283419905035, "compression_ratio": 1.4404761904761905, "no_speech_prob": 4.835758591070771e-05, "transient": false}, {"id": 77, "seek": 45014, "start": 473.71999999999997, "end": 476.08, "text": " search system.", "tokens": [3164, 1185, 13], "temperature": 0.0, "avg_logprob": -0.32167283419905035, "compression_ratio": 1.4404761904761905, "no_speech_prob": 4.835758591070771e-05, "transient": false}, {"id": 78, "seek": 47608, "start": 476.08, "end": 480.28, "text": " And maybe we will improve it in the future.", "tokens": [400, 1310, 321, 486, 3470, 309, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.41929615668530734, "compression_ratio": 1.375, "no_speech_prob": 8.99459482752718e-05, "transient": false}, {"id": 79, "seek": 47608, "start": 480.28, "end": 488.24, "text": " So but next stage, we want to automate our viscous transcription with GPT magic to allow", "tokens": [407, 457, 958, 3233, 11, 321, 528, 281, 31605, 527, 1452, 39939, 35288, 365, 26039, 51, 5585, 281, 2089], "temperature": 0.0, "avg_logprob": -0.41929615668530734, "compression_ratio": 1.375, "no_speech_prob": 8.99459482752718e-05, "transient": false}, {"id": 80, "seek": 47608, "start": 488.24, "end": 498.84, "text": " users to add videos, video pages by themselves, a bit improve our usability.", "tokens": [5022, 281, 909, 2145, 11, 960, 7183, 538, 2969, 11, 257, 857, 3470, 527, 46878, 13], "temperature": 0.0, "avg_logprob": -0.41929615668530734, "compression_ratio": 1.375, "no_speech_prob": 8.99459482752718e-05, "transient": false}, {"id": 81, "seek": 49884, "start": 498.84, "end": 508.15999999999997, "text": " And more than we will try to make your answer your questions using data stored in our knowledge", "tokens": [400, 544, 813, 321, 486, 853, 281, 652, 428, 1867, 428, 1651, 1228, 1412, 12187, 294, 527, 3601], "temperature": 0.0, "avg_logprob": -0.43704698143935783, "compression_ratio": 1.4393939393939394, "no_speech_prob": 2.1578744053840637e-05, "transient": false}, {"id": 82, "seek": 49884, "start": 508.15999999999997, "end": 509.67999999999995, "text": " management.", "tokens": [4592, 13], "temperature": 0.0, "avg_logprob": -0.43704698143935783, "compression_ratio": 1.4393939393939394, "no_speech_prob": 2.1578744053840637e-05, "transient": false}, {"id": 83, "seek": 49884, "start": 509.67999999999995, "end": 511.28, "text": " That's all for today.", "tokens": [663, 311, 439, 337, 965, 13], "temperature": 0.0, "avg_logprob": -0.43704698143935783, "compression_ratio": 1.4393939393939394, "no_speech_prob": 2.1578744053840637e-05, "transient": false}, {"id": 84, "seek": 49884, "start": 511.28, "end": 514.76, "text": " By the way, we are going to participate in our Zagreco-Tron.", "tokens": [3146, 264, 636, 11, 321, 366, 516, 281, 8197, 294, 527, 1176, 559, 265, 1291, 12, 51, 2044, 13], "temperature": 0.0, "avg_logprob": -0.43704698143935783, "compression_ratio": 1.4393939393939394, "no_speech_prob": 2.1578744053840637e-05, "transient": false}, {"id": 85, "seek": 49884, "start": 514.76, "end": 517.8399999999999, "text": " So if you're interested, please join our team.", "tokens": [407, 498, 291, 434, 3102, 11, 1767, 3917, 527, 1469, 13], "temperature": 0.0, "avg_logprob": -0.43704698143935783, "compression_ratio": 1.4393939393939394, "no_speech_prob": 2.1578744053840637e-05, "transient": false}, {"id": 86, "seek": 49884, "start": 517.8399999999999, "end": 521.76, "text": " We have a huge project moving forward.", "tokens": [492, 362, 257, 2603, 1716, 2684, 2128, 13], "temperature": 0.0, "avg_logprob": -0.43704698143935783, "compression_ratio": 1.4393939393939394, "no_speech_prob": 2.1578744053840637e-05, "transient": false}, {"id": 87, "seek": 52176, "start": 521.76, "end": 529.18, "text": " See you.", "tokens": [50364, 3008, 291, 13, 50735], "temperature": 1.0, "avg_logprob": -1.2016736666361492, "compression_ratio": 0.5, "no_speech_prob": 3.488983929855749e-05, "transient": false}], "text": "Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia. So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some structure here, and whatever, just editor for the small pages. So we converted demos of a few teams into this Wikipedia format using Whisper and some magic from GPT to make them much more readable. But what is more important, we allow them to be searchable by semantic search. So we use a multilingual semantic search, and I will show you how it works. We have some project, Ask Quran, let's try to formulate it in Russian, like a questions about God. Yeah, we have Ask Quran project. It's more delay on loading. I will skip a bit. Seems like it's quite a good match about questions about God in Russian to the English text. So let me show you how our system, our Wikipedias works with video. So when video is playing, we highlight the part of the video, or you can jump to the part of the video from the video. We could jump to the timestamp in the video, clicking to the text, or you can switch the video part and see which part of the text it is. Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search. So I will try to write in Spanish, not as personales, which is personal nouns, and see what we get. So we have a daily journaling app presentation. To speed up, I will click here. In conversational daily journaling experience, where we hope to enable users to write your way to a better day. That seems quite a good match for the questions, not as personales. Well, enough for the glamour, back to slides. Before jumping into the technical details, I would like to say that this knowledge management market is huge. It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two digits figures, depending on which report you read. And it's quite a good trend to disrupt this market by new AI solutions or AI technologies, like our IHANA. So that could not be just a fancy toy with technology, but it could be a part of the business because this solution is applicable in every enterprise environment. Well, then it could help the people who cannot listen to or watch videos to read information. And I would say you can help busy managers to spend some time reading instead of watching videos. Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the webcasts, but they are not searchable, so I guess it's not accessible. Well, how it was done. As a base, we use an open source knowledge management system, Wikijs. It's quite commercially used and popular, written in JavaScript. It already supports some amount of search engines that you can set up and it will use as a search engine. So we forked it and implement our OWASP, a basic combination of coherent and quadrant services. So now our fork support set up all these coherent and quadrant search systems with EPI case, and that's exactly how our demo worked. Internally from the technical perspective, we convert each paragraph of the text on each page into the vector using coherent embedding multilingual model. Then we just index and store them in the vector database provided by quadrant. And when we have a query, we do almost the same. We convert it to the embedding and after that we search to the nearest embeddings in the quadrant vector database. And magically, our link text to these vectors are quite close to the answers to the questions we put into the query. That's a kind of the AI magic, I guess. So the future work, first of all, current implementation is super naive of search system. Maybe we should use a bit more complex algorithm. But the funny, it takes like 300 lines of code to implement this magically high quality search system. And maybe we will improve it in the future. So but next stage, we want to automate our viscous transcription with GPT magic to allow users to add videos, video pages by themselves, a bit improve our usability. And more than we will try to make your answer your questions using data stored in our knowledge management. That's all for today. By the way, we are going to participate in our Zagreco-Tron. So if you're interested, please join our team. We have a huge project moving forward. See you."}