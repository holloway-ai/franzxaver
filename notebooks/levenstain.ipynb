{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm Colleen Farley.\n",
      " I live in Miami, Florida.\n",
      " I am the author of The Shape of Data, which will be coming out this next year.\n",
      " And I've focused a lot on public health, social sciences, and a lot of mathematical tools that\n",
      " can be used to study both of these.\n",
      " So let's go through some of the tools.\n",
      " We're not going to get into details here too much.\n",
      " Just kind of what the tools are and how we can use them.\n",
      " So the first tool is something called persistent homology, which comes from a branch of math\n",
      " called topology.\n",
      " This branch is not real concerned with how your data is put together in the local sense.\n",
      " It's more about how your data is connected.\n",
      " So basically we create a series of distance-based objects and look at features across them.\n",
      " This does connect back to geometry and the curvature and the distances that you define\n",
      " in your data.\n",
      " But it's a little broader of an overview.\n",
      " The next tool is Furman-Ritchie curvature, which is a network science metric.\n",
      " We just had a great two presentations on network science and how useful it is.\n",
      " This tool measures the curvature on the graph.\n",
      " So in physics, when we have a lot of matter in the same spot, it pulls everything towards\n",
      " it.\n",
      " So black holes are going to pull planets towards it, pull your spaceship towards it.\n",
      " And the same thing happens in networks.\n",
      " When we have a lot of things that are connected in the same area of the graph, processes that\n",
      " happen on a graph tend to get pulled into there.\n",
      " So if you're looking at marketing campaigns, it tends to clump around that area.\n",
      " If you're looking at epidemics, that's typically where you're going to find a lot of cases.\n",
      " And in general, this is a very useful tool for understanding change points and behavior\n",
      " over time on graphs.\n",
      " There's a newer branch of machine learning that's been really useful.\n",
      " Generative AI and some of the geometry tools that evolve these creations have been playing\n",
      " a major role in having people being able to develop things through AI without actually\n",
      " having to know much coding.\n",
      " So basically there's a deep learning algorithm that can take your text and create an image.\n",
      " And there are tools in geometry that are able to evolve and kind of massage out what you\n",
      " want from the image.\n",
      " This is an example on the left of a place that does not actually exist.\n",
      " But I created it with one of the open source tools out there.\n",
      " And this can be really useful, as we'll see.\n",
      " So let's get into the applications.\n",
      " So a few of the projects that I've used persistent homology on are looking at urban growth tipping\n",
      " points.\n",
      " So as cities grow, especially if there's not a lot of infrastructure or planning going\n",
      " on when cities start evolving, we can run into problems with infrastructure, we can\n",
      " run into problems with public health and sanitation, and we can end up with environmental issues\n",
      " if there's a tipping point that results in a lot of growth without enough time to build\n",
      " all of the things needed to keep a city running.\n",
      " We can also look at climate change and understand when there are points of no return, when the\n",
      " climate is going to tip, when populations aren't safe living in an area anymore.\n",
      " And this is really important to be able to relocate populations before a disaster happens\n",
      " in their area or before the area is unlivable, such as barrier islands, communities on rivers,\n",
      " areas near the Sahara, which are dealing with a lot of deforestation and evolution into\n",
      " desert climates.\n",
      " In addition, with the recent COVID epidemic, this is also really important in understanding\n",
      " epidemic growth so that we can position resources, especially in communities that may not have\n",
      " a lot of starting resources.\n",
      " Being able to understand where and when we need to place these resources can go a long\n",
      " way in preventing the spread of an epidemic and limiting the loss of life during one.\n",
      " Norman Ricci curvature is another tool that I use a lot in my work.\n",
      " Right now, there's a huge crisis in food prices.\n",
      " So this is impacting a lot of communities.\n",
      " Middle class populations are finding themselves being driven back into poverty with rising\n",
      " food prices.\n",
      " And there's a lot of food insecurity that has started, especially since the Ukraine\n",
      " conflict.\n",
      " So understanding when prices are going to change, what prices are going to change, and\n",
      " where they're going to change allows social programs to be able to meet these needs.\n",
      " It allows food aid programs to be able to know how much aid they may need to meet the\n",
      " need of the communities, as well as what governments can do locally to reduce the price of food\n",
      " and help populations get through rough patches.\n",
      " One of the first times that I used these models on a project was during the DR Congo 2018\n",
      " Ebola outbreak.\n",
      " The outbreak happened in an under-resourced area of the country, and getting resources\n",
      " in the areas impacted was difficult for NGOs and for local responders.\n",
      " And being able to understand where and when the next wave of the epidemic would hit helped\n",
      " be able to position those resources to respond accordingly and in areas of greatest need.\n",
      " There's another use, more common in marketing, but also with behavior change, of understanding\n",
      " where in a social network or a geographic network an intervention can be done.\n",
      " So say you want to have a smoking cessation program or a sexual health education program\n",
      " that tips behavior change at a population level.\n",
      " Understanding how your network is put together and who has the influence in the network to\n",
      " be able to impact change on a population level without having the resources to intervene\n",
      " on everyone, having these type of tools is really, really useful to be able to selectively\n",
      " target different populations or different individuals within the population to affect\n",
      " the system level change.\n",
      " So last we have generative AI.\n",
      " One of the things that I've found is that this is very useful in public health education.\n",
      " So during the COVID outbreak, we had mass campaigns, vaccination campaigns to try to\n",
      " help improve population health and protect vulnerable populations.\n",
      " However, there are a lot of caveats with doing public health in remote places with certain\n",
      " populations.\n",
      " Being able to make something culturally relevant is really important.\n",
      " To people who look like the community, being the image or the person speaking is really\n",
      " important to be relatable.\n",
      " And generative AI allows us to create culturally relevant images for these campaigns.\n",
      " In addition, they're really useful as a way to be able to do this with under-resourced\n",
      " areas, areas that don't have maybe as much infrastructure around creating videos, creating\n",
      " image campaigns, creating public service announcements.\n",
      " Having this technology allows the resources to go further.\n",
      " Long time ago, when I was working on HIV education in South Africa, there wasn't a lot to go\n",
      " on and a lot of it had to be developed on the fly.\n",
      " And that's very hard to do when you're in a rural community.\n",
      " It's a lot of finding people, positioning, whatever skit or whatever materials need to\n",
      " be created, actually creating those materials.\n",
      " It used to take a lot of time 20 years ago.\n",
      " But now with generative AI, we can tell our algorithm what we want to generate, generate\n",
      " 10 images or 100 images, and then look through them for what's most likely to resonate with\n",
      " the population.\n",
      " And that's really impactful.\n",
      " And I think in the future, we're going to be seeing a lot more of these technologies\n",
      " being leveraged in fields that might not have used these, in marketing, in public health,\n",
      " in social program awareness campaigns, pretty much anything that needs to get the information\n",
      " out, particularly in a visual manner.\n",
      " So a lot of areas of the world have a lot of local languages.\n",
      " And somebody who is a native speaker of that language might know some of the more dominant\n",
      " language messages can get lost in translation.\n",
      " Or there might be a need to hire a lot of translators to get the information out there.\n",
      " And this technology promises to really help solve a lot of those barriers to public health\n",
      " interventions, social policy announcements, even just getting information about what resources\n",
      " exist to these populations.\n",
      " So one of the things that I'm really passionate about is the need for representation.\n",
      " So if we don't collect data on people who look a certain way or speak a certain language\n",
      " or come from a certain area of the country or the world, we don't have that data for\n",
      " our algorithms to learn from.\n",
      " And those populations may not have their needs met.\n",
      " If we don't have that representation on tech teams, we don't have that perspective of how\n",
      " there might be barriers to adopting the technology or having the technology work correctly within\n",
      " that population.\n",
      " So it's really important that we consider different cultures and different subgroups\n",
      " so that everyone can participate.\n",
      " I think we're at a tipping point within tech of those who are benefiting a lot from it\n",
      " and those that haven't benefited much.\n",
      " And if we keep going this way and some populations have a lot more representation than others,\n",
      " I think we're going to be seeing more and more division along the lines of technology.\n",
      " So that's really important, especially within social good projects that we have representation\n",
      " from the community, that the community has ownership in data, input into how this is\n",
      " going to be used or impact their community.\n",
      " So there are a lot of good resources out there.\n",
      " I'm going to make this presentation public on my LinkedIn.\n",
      " So feel free to reach out to me.\n",
      " Feel free to look at these resources for the tools that are out there.\n",
      " Persistent Homology has a lot of packages that exist in R and Python.\n",
      " Text to image, OpenAI and Night Cafe have some really great tools.\n",
      " And for Forman Ritchie Curvature, I have code in Python and in R for different network applications.\n",
      " And feel free to reach out to me.\n"
     ]
    }
   ],
   "source": [
    "# input_path = Path('input.txt')\n",
    "# result_path = Path('result.md')\n",
    "input_path = Path('1_transcript.txt')\n",
    "result_path = Path('2_result.md')\n",
    "\n",
    "with input_path.open() as f:\n",
    "    input_lines = \"\".join(f.readlines())\n",
    "with result_path.open() as f:\n",
    "    result_lines = \"\".join(f.readlines())\n",
    "    \n",
    "print (input_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm Colleen Farley. I live in Miami, Florida. I am the author of *The Shape of Data*, which will be coming out this next year. And I've focused a lot on public health, social sciences, and a lot of mathematical tools that can be used to study both of these. So let's go through some of the tools. We're not going to get into details here too much. Just kind of what the tools are and how we can use them. So the first tool is something called persistent homology, which comes from a branch of math called topology. This branch is not real concerned with how your data is put together in the local sense. It's more about how your data is connected. So basically we create a series of distance-based objects and look at features across them. This does connect back to geometry and the curvature and the distances that you define in your data. But it's a little broader of an overview. The next tool is Furman-Ritchie curvature, which is a network science metric. We just had a great two presentations on network science and how useful it is. This tool measures the curvature on the graph. So in physics, when we have a lot of matter in the same spot, it pulls everything towards it. So black holes are going to pull planets towards it, pull your spaceship towards it. And the same thing happens in networks. When we have a lot of things that are connected in the same area of the graph, processes that happen on a graph tend to get pulled into there. So if you're looking at marketing campaigns, it tends to clump around that area. If you're looking at epidemics, that's typically where you're going to find a lot of cases. And in general, this is a very useful tool for understanding change points and behavior over time on graphs. There's a newer branch of machine learning that's been really useful. Generative AI and some of the geometry tools that evolve these creations have been playing a major role in having people being able to develop things through AI without actually having to know much coding. So basically there's a deep learning algorithm that can take your text and create an image. And there are tools in geometry that are able to evolve and kind of massage out what you want from the image. This is an example on the left of a place that does not actually exist. But I created it with one of the open source tools out there. And this can be really useful, as we'll see. So let's get into the applications. So a few of the projects that I've used persistent homology on are looking at urban growth tipping points. So as cities grow, especially if there's not a lot of infrastructure or planning going on when cities start evolving, we can run into problems with infrastructure, we can run into problems with public health and sanitation, and we can end up with environmental issues if there's a tipping point that results in a lot of growth without enough time to build all of the things needed to keep a city running. We can also look at climate change and understand when there are points of no return, when the climate is going to tip, when populations aren't safe living in an area anymore. And this is really important to be able to relocate populations before a disaster happens in their area or before the area is unlivable, such as barrier islands, communities on rivers, areas near the Sahara, which are dealing with a lot of deforestation and evolution into desert climates. In addition, with the recent COVID epidemic, this is also really important in understanding epidemic growth so that we can position resources, especially in communities that may not have a lot of starting resources. Being able to understand where and when we need to place these resources can go a long way in preventing the spread of an epidemic and limiting the loss of life during one. Norman Ricci curvature is another tool that I use a lot in my work. Right now, there's a huge crisis in food prices. So this is impacting a lot of communities. Middle class populations are finding themselves being driven back into poverty with rising food prices. And there's a lot of food insecurity that has started, especially since the Ukraine conflict. So understanding when prices are going to change, what prices are going to change, and where they're going to change allows social programs to be able to meet these needs. It allows food aid programs to be able to know how much aid they may need to meet the need of the communities, as well as what governments can do locally to reduce the price of food and help populations get through rough patches. One of the first times that I used these models on a project was during the DR Congo 2018 Ebola outbreak. The outbreak happened in an under-resourced area of the country, and getting resources in the areas impacted was difficult for NGOs and for local responders. And being able to understand where and when the next wave of the epidemic would hit helped be able to position those resources to respond accordingly and in areas of greatest need. There's another use, more common in marketing, but also with behavior change, of understanding where in a social network or a geographic network an intervention can be done. So say you want to have a smoking cessation program or a sexual health education program that tips behavior change at a population level. Understanding how your network is put together and who has the influence in the network to be able to impact change on a population level without having the resources to intervene on everyone, having these type of tools is really, really useful to be able to selectively target different populations or different individuals within the population to affect the system level change. So last we have generative AI. One of the things that I've found is that this is very useful in public health education. So during the COVID outbreak, we had mass campaigns, vaccination campaigns to try to help improve population health and protect vulnerable populations. However, there are a lot of caveats with doing public health in remote places with certain populations. Being able to make something culturally relevant is really important. To people who look like the community, being the image or the person speaking is really important to be relatable. And generative AI allows us to create culturally relevant images for these campaigns. In addition, they're really useful as a way to be able to do this with under-resourced areas, areas that don't have maybe as much infrastructure around creating videos, creating image campaigns, creating public service announcements. Having this technology allows the resources to go further. Long time ago, when I was working on HIV education in South Africa, there wasn't a lot to go on and a lot of it had to be developed on the fly. And that's very hard to do when you're in a rural community. It's a lot of finding people, positioning, whatever skit or whatever materials need to be created, actually creating those materials. It used to take a lot of time 20 years ago. But now with generative AI, we can tell our algorithm what we want to generate, generate 10 images or 100 images, and then look through them for what's most likely to resonate with the population. And that's really impactful. And I think in the future, we're going to be seeing a lot more of these technologies being leveraged in fields that might not have used these, in marketing, in public health, in social program awareness campaigns, pretty much anything that needs to get the information out, particularly in a visual manner. So a lot of areas of the world have a lot of local languages. And somebody who is a native speaker of that language might know some of the more dominant language messages can get lost in translation. Or there might be a need to hire a lot of translators to get the information out there. And this technology promises to really help solve a lot of those barriers to public health interventions, social policy announcements, even just getting information about what resources exist to these populations. So one of the things that I'm really passionate about is the need for representation. So if we don't collect data on people who look a certain way or speak a certain language or come from a certain area of the country or the world, we don't have that data for our algorithms to learn from. And those populations may not have their needs met. If we don't have that representation on tech teams, we don't have that perspective of how there might be barriers to adopting the technology or having the technology work correctly within that population. So it's really important that we consider different cultures and different subgroups so that everyone can participate. I think we're at a tipping point within tech of those who are benefiting a lot from it and those that haven't benefited much. And if we keep going this way and some populations have a lot more representation than others, I think we're going to be seeing more and more division along the lines of technology. So that's really important, especially within social good projects that we have representation from the community, that the community has ownership in data, input into how this is going to be used or impact their community. So there are a lot of good resources out there. I'm going to make this presentation public on my LinkedIn. So feel free to reach out to me. Feel free to look at these resources for the tools that are out there. - Persistent Homology has a lot of packages that exist in \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_img_str = r\"\\n*!\\[\\{\\~.*\\}\\]\\(.*\\)\\n*\"\n",
    "re_ts_str = r\" *\\{\\~\\d+\\.\\d+\\} *\"\n",
    "re_header_str = r\"#.*\\n+\"\n",
    "re_space_str = r\" {2,}\"\n",
    "re_new_str = r\"\\n+\"\n",
    "re_addons = re.compile(f\"({re_img_str}|{re_ts_str}|{re_header_str})\")\n",
    "re.sub(r\"[ \\n]+\",\" \",re_addons.sub(\" \",result_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873343151693668"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clean_result = re_addons.sub(\"\",result_lines).replace('\\n', ' ').replace('  ', ' ')\n",
    "clean_result= re.sub(r\"[ \\n]+\",\" \",re_addons.sub(\" \",result_lines))\n",
    "clean_input = input_lines.replace('\\n', ' ').replace('  ', ' ')\n",
    "toc_input = re.split(r\"([~\\w']+)\", clean_input)\n",
    "toc_res = re.split(r\"([~\\w']+)\", clean_result)\n",
    "lev.ratio(clean_input, clean_result)\n",
    "lev.ratio(toc_input, toc_res)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\n",
    "        \"../test_data/pydata/header\"\n",
    "    )\n",
    "transcript_path = data_path / \"transcript.json\"\n",
    "with transcript_path.open() as f:\n",
    "    transcript = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_words = re.compile(r\"([~\\w']+)\")\n",
    "re_words = re.compile(r\"([~\\w']+|[.,\\?!])\")\n",
    "\n",
    "transcript_words = []\n",
    "transcript_word2segment = []\n",
    "transcript_segment2word = {}\n",
    "for sed_n, seg in enumerate (transcript[\"segments\"]):\n",
    "    transcript_segment2word[sed_n] = len(transcript_words)\n",
    "    for word in re_words.split(seg[\"text\"]):\n",
    "        if word:\n",
    "            transcript_words.append(word)\n",
    "            transcript_word2segment.append(sed_n)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Everybody has a turn back moment.\n",
       "\n",
       "\n",
       "\n",
       "You have a moment where you could go forward or you can give up. But the thing you have to keep in mind before you give up is that if you give up, the guarantee is it will never happen. That's the guarantee of quitting. That it will never happen no way under the sun. The only way the possibility remains that it can happen is if you never give up no matter what."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_path = data_path / \"0:0_result.md\"\n",
    "with result_path.open() as f:\n",
    "    result_text = \"\\n\".join(f.readlines()[2:])\n",
    "    \n",
    "Markdown(result_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Everybody has a turn back moment. You have a moment where you could go forward\n",
      "1: or you can give up. But the thing you have to keep in mind before you give up\n",
      "2: is that if you give up, the guarantee is it will never happen. That's the guarantee\n",
      "3: of quitting. That it will never happen no way under the sun. The only way the\n",
      "4: possibility remains that it can happen is if you never give up no matter what.\n"
     ]
    }
   ],
   "source": [
    "for sed_n, seg in enumerate (transcript[\"segments\"]):\n",
    "    print (f\"{sed_n}:{seg['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_text = \"\"\"## New Header\n",
    "\n",
    "But the thing you have to keep in mind before you give up as that if you give up, the guarantee is it will never happen.\n",
    "\n",
    "## Second Header\n",
    "\n",
    "That's the guarantee at quitting. That it will never happen no way under the sun.\n",
    "\n",
    "## Third Header\n",
    "\"\"\"\n",
    "markdown_words = re_words.split(result_text)\n",
    "start_segment = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_paragraphs =  result_text.split(\"\\n\\n\")\n",
    "markdown_words = []\n",
    "markdown_word2paragraph = []\n",
    "for par_n, par in enumerate(markdown_paragraphs):\n",
    "    is_header = par.startswith(\"#\")\n",
    "    for word in re_words.split(par):\n",
    "        if word:\n",
    "            markdown_words.append(word)\n",
    "            markdown_word2paragraph.append((par_n,is_header))\n",
    "    #markdown_words.append(\"\\\\n\\\\n\")\n",
    "    #markdown_word2paragraph.append((par_n,is_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('replace', 0, 2, 0, 2),\n",
       " ('equal', 2, 3, 2, 3),\n",
       " ('replace', 3, 4, 3, 4),\n",
       " ('delete', 4, 43, 4, 4),\n",
       " ('equal', 43, 69, 4, 30),\n",
       " ('replace', 69, 70, 30, 31),\n",
       " ('equal', 70, 96, 31, 57),\n",
       " ('insert', 96, 96, 57, 59),\n",
       " ('equal', 96, 97, 59, 60),\n",
       " ('insert', 97, 97, 60, 61),\n",
       " ('equal', 97, 103, 61, 67),\n",
       " ('replace', 103, 104, 67, 68),\n",
       " ('equal', 104, 128, 68, 92),\n",
       " ('replace', 128, 130, 92, 94),\n",
       " ('equal', 130, 131, 94, 95),\n",
       " ('replace', 131, 133, 95, 97),\n",
       " ('delete', 133, 167, 97, 97)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = lev.opcodes(transcript_words, markdown_words)\n",
    "transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace : '  Everybody '->' ## New '\n",
      "equal : '   '->'   '\n",
      "replace : ' has '->' Header '\n",
      "delete : '  a turn back moment. You have a moment where you could go forward or you can give up.  '->'  '\n",
      "equal : ' But the thing you have to keep in mind before you give up  '->' But the thing you have to keep in mind before you give up  '\n",
      "replace : ' is '->' as '\n",
      "equal : '  that if you give up, the guarantee is it will never happen. '->'  that if you give up, the guarantee is it will never happen. '\n",
      "insert : '  '->' ## Second '\n",
      "equal : '   '->'   '\n",
      "insert : '  '->' Header '\n",
      "equal : ' That's the guarantee  '->' That's the guarantee  '\n",
      "replace : ' of '->' at '\n",
      "equal : '  quitting. That it will never happen no way under the sun. '->'  quitting. That it will never happen no way under the sun. '\n",
      "replace : '  The '->' ## Third '\n",
      "equal : '   '->'   '\n",
      "replace : ' only  '->' Header\n",
      " '\n",
      "delete : ' way the possibility remains that it can happen is if you never give up no matter what. '->'  '\n"
     ]
    }
   ],
   "source": [
    "for op in transform:\n",
    "    print (op[0],\": '\",\"\".join(transcript_words[op[1]:op[2]]),\"'->'\",\"\".join(markdown_words[op[3]:op[4]]) ,\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" or you can give up. But the thing you have to keep in mind before you give up is that if you give up, the guarantee is it will never happen. That's the guarantee of quitting. That it will never happen no way under the sun. The only way the\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_start = transcript_segment2word[start_segment]\n",
    "transcript_end = min( transcript_start + len (markdown_words),len(transcript_words)-1)\n",
    "print (transcript_end, transcript_word2segment[transcript_end])\n",
    "if transcript_word2segment[transcript_end] < len(transcript_segment2word) -1:\n",
    "    transcript_end = transcript_segment2word[transcript_word2segment[transcript_end] + 1]\n",
    "else:\n",
    "    transcript_end = len(transcript_words)\n",
    "\n",
    "\"\".join(transcript_words[transcript_start: transcript_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0:' Everybody has a turn back moment. You have a moment where you could go forward'\n",
      "1 -> 31:' or you can give up. But the thing you have to keep in mind before you give up'\n",
      "2 -> 68:' is that if you give up, the guarantee is it will never happen. That's the guarantee'\n",
      "3 -> 102:' of quitting. That it will never happen no way under the sun. The only way the'\n",
      "4 -> 136:' possibility remains that it can happen is if you never give up no matter what.'\n"
     ]
    }
   ],
   "source": [
    "for sed_n, seg in enumerate (transcript[\"segments\"]):\n",
    "    print (f\"{sed_n} -> {transcript_segment2word[sed_n]}:'{seg['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert [43:43]->[0:4]:\n",
      "   ||\n",
      "   |## |New| |Header|\n",
      "equal [43:69]->[4:30]:\n",
      "   |But| |the| |thing| |you| |have| |to| |keep| |in| |mind| |before| |you| |give| |up| |\n",
      "   |But| |the| |thing| |you| |have| |to| |keep| |in| |mind| |before| |you| |give| |up| |\n",
      "replace [69:70]->[30:31]:\n",
      "   |is|\n",
      "   |as|\n",
      "equal [70:96]->[31:57]:\n",
      "   | |that| |if| |you| |give| |up|,| |the| |guarantee| |is| |it| |will| |never| |happen|.|\n",
      "   | |that| |if| |you| |give| |up|,| |the| |guarantee| |is| |it| |will| |never| |happen|.|\n",
      "insert [96:96]->[57:59]:\n",
      "   ||\n",
      "   |## |Second|\n",
      "equal [96:97]->[59:60]:\n",
      "   | |\n",
      "   | |\n",
      "insert [97:97]->[60:61]:\n",
      "   ||\n",
      "   |Header|\n",
      "equal [97:103]->[61:67]:\n",
      "   |That's| |the| |guarantee| |\n",
      "   |That's| |the| |guarantee| |\n",
      "replace [103:104]->[67:68]:\n",
      "   |of|\n",
      "   |at|\n",
      "equal [104:128]->[68:92]:\n",
      "   | |quitting|.| |That| |it| |will| |never| |happen| |no| |way| |under| |the| |sun|.|\n",
      "   | |quitting|.| |That| |it| |will| |never| |happen| |no| |way| |under| |the| |sun|.|\n",
      "insert [128:128]->[92:97]:\n",
      "   ||\n",
      "   |## |Third| |Header|\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "transcript_start, transcript_end = 43, 128\n",
    "#transcript_start =  41\n",
    "transform = lev.opcodes(transcript_words[transcript_start: transcript_end], markdown_words)\n",
    "\n",
    "for op in transform:\n",
    "    print (f\"{op[0]} [{op[1]+transcript_start}:{op[2]+transcript_start}]->[{op[3]}:{op[4]}]:\")\n",
    "    print (f\"   |{'|'.join(transcript_words[op[1]+transcript_start:op[2]+transcript_start])}|\")\n",
    "    print (f\"   |{'|'.join(markdown_words[op[3]:op[4]])}|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## New Header\n",
      "\n",
      "{1}But the thing you have to keep in mind before you give up{1}{2} as that if you give up, the guarantee is it will never happen.{2}\n",
      "\n",
      "{2}That's the guarantee{2}{3} at quitting. That it will never happen no way under the sun.{3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraph_threshold, header_threshold = (0,0) if transcript_end == len(transcript_words) else (10,50)\n",
    "\n",
    "result_tokens = []\n",
    "last_markdown_paragraph = 0\n",
    "last_transcript_segment  = None\n",
    "last_transcript_pos = None\n",
    "last_markdown_pos = None\n",
    "last_is_header  = False\n",
    "for op, t_from, t_to, m_from, m_to in transform:\n",
    "    if op !=\"delete\":\n",
    "        for i in range(0, m_to - m_from):\n",
    "            \n",
    "            if op==\"insert\":\n",
    "                transcript_pos = t_from + transcript_start \n",
    "                new_transcript_segment = None\n",
    "            else:\n",
    "                transcript_pos = t_from + i + transcript_start \n",
    "                new_transcript_segment = transcript_word2segment[transcript_pos]\n",
    "                \n",
    "            markdown_pos = i+ m_from\n",
    "            new_markdown_paragraph = markdown_word2paragraph[markdown_pos][0]\n",
    "            current_is_header = markdown_word2paragraph[markdown_pos][1]\n",
    "            \n",
    "            is_segment_change = new_transcript_segment != last_transcript_segment\n",
    "            is_new_paragraph = last_markdown_paragraph != new_markdown_paragraph\n",
    "            \n",
    "            if is_new_paragraph and current_is_header and (markdown_pos> len(markdown_words) - header_threshold):\n",
    "                break\n",
    "            if is_new_paragraph and not current_is_header and (markdown_pos> len(markdown_words) - paragraph_threshold):\n",
    "                break               \n",
    "         \n",
    "            if not last_transcript_segment is None and (is_segment_change or is_new_paragraph) and not last_is_header:\n",
    "                result_tokens.append(f\"{{{last_transcript_segment}}}\")\n",
    "            if is_new_paragraph:\n",
    "                result_tokens.append(\"\\n\\n\")\n",
    "            if not new_transcript_segment is None and (is_segment_change or is_new_paragraph) and not current_is_header:\n",
    "                    result_tokens.append(f\"{{{new_transcript_segment}}}\")\n",
    "            result_tokens.append(markdown_words[markdown_pos])\n",
    "            if op == \"insert\":\n",
    "                last_transcript_segment = None\n",
    "            last_transcript_segment = new_transcript_segment\n",
    "            last_markdown_paragraph = new_markdown_paragraph\n",
    "            last_is_header = current_is_header\n",
    "            last_markdown_pos = markdown_pos\n",
    "            if op == \"equal\":\n",
    "                last_transcript_pos = transcript_pos\n",
    "                \n",
    "            \n",
    "if last_transcript_segment is not None and not last_is_header:\n",
    "    result_tokens.append(f\"{{{last_transcript_segment}}}\")\n",
    "    last_transcript_segment = None  \n",
    "\n",
    "print (\"\".join(result_tokens))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Third Header\n",
      "\n",
      " The only way the possibility remains that it can happen is if you never give up no matter what.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"\".join(markdown_words[last_markdown_pos+1:]))\n",
    "print (\"\".join(transcript_words[last_transcript_pos+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Twenty-one',\n",
       " 'twenty-two',\n",
       " 'twenty-three',\n",
       " 'twenty-four',\n",
       " 'twenty-five',\n",
       " 'Twenty-six',\n",
       " 'twenty-seven',\n",
       " 'twenty-eight',\n",
       " 'twenty-nine',\n",
       " 'thirty',\n",
       " '']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = \"Twenty-one, twenty-two, twenty-three, twenty-four, twenty-five. Twenty-six, twenty-seven, twenty-eight, twenty-nine, thirty,\"\n",
    "\n",
    "re.split(r\"[^\\w-]+\", s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jkdsfldsj\n",
    "- oo\n",
    "- vvv\n",
    "- gfd\n",
    "\n",
    "eewr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
