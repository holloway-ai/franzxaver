{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!export DEBIAN_FRONTEND=noninteractive && sudo apt-get update && sudo apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"OPENAI_API_KEY\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generic] Extracting URL: https://storage.googleapis.com/lablab-video-submissions/clect7lts0000356kwijsh28b%2Fraw%2Fsubmiss...00sq3b6kem8th9uo.mp4\n",
      "[generic] clect7lts0000356kwijsh28b/raw/submission-video-x-clect7lts0000356kwijsh28b-clf8o1gq900sq3b6kem8th9uo: Downloading webpage\n",
      "[info] clect7lts0000356kwijsh28b/raw/submission-video-x-clect7lts0000356kwijsh28b-clf8o1gq900sq3b6kem8th9uo: Downloading 1 format(s): mp4\n",
      "[download] Destination: audio\n",
      "[download] 100% of  166.86MiB in 00:00:01 at 96.69MiB/s  \n",
      "[ExtractAudio] Destination: audio.mp3\n",
      "Deleting original file audio (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "url=\"https://storage.googleapis.com/lablab-video-submissions/clect7lts0000356kwijsh28b%2Fraw%2Fsubmission-video-x-clect7lts0000356kwijsh28b-clf8o1gq900sq3b6kem8th9uo.mp4\"\n",
    "\n",
    "\n",
    "URLS = [url]\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    \"outtmpl\": \"audio\",\n",
    "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
    "    'postprocessors': [{  # Extract audio using ffmpeg\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "    }]\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    error_code = ydl.download(URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'json', 'text', 'vtt', 'srt', 'verbose_json'\n",
    "\n",
    "with open(\"audio.mp3\", \"rb\") as audio_file: \n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, response_format=\"verbose_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7f53df0b7e20> JSON: {\n",
       "  \"duration\": 524.07,\n",
       "  \"language\": \"english\",\n",
       "  \"segments\": [\n",
       "    {\n",
       "      \"avg_logprob\": -0.34817066899052374,\n",
       "      \"compression_ratio\": 1.4768518518518519,\n",
       "      \"end\": 8.84,\n",
       "      \"id\": 0,\n",
       "      \"no_speech_prob\": 0.3598945140838623,\n",
       "      \"seek\": 0,\n",
       "      \"start\": 0.0,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise\",\n",
       "      \"tokens\": [\n",
       "        2421,\n",
       "        11,\n",
       "        286,\n",
       "        478,\n",
       "        510,\n",
       "        281,\n",
       "        5366,\n",
       "        281,\n",
       "        291,\n",
       "        11,\n",
       "        291,\n",
       "        366,\n",
       "        527,\n",
       "        700,\n",
       "        490,\n",
       "        1379,\n",
       "        46731,\n",
       "        320,\n",
       "        1605,\n",
       "        295,\n",
       "        26696\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.34817066899052374,\n",
       "      \"compression_ratio\": 1.4768518518518519,\n",
       "      \"end\": 9.84,\n",
       "      \"id\": 1,\n",
       "      \"no_speech_prob\": 0.3598945140838623,\n",
       "      \"seek\": 0,\n",
       "      \"start\": 8.84,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Assistants.\",\n",
       "      \"tokens\": [\n",
       "        49633,\n",
       "        1719,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.34817066899052374,\n",
       "      \"compression_ratio\": 1.4768518518518519,\n",
       "      \"end\": 16.56,\n",
       "      \"id\": 2,\n",
       "      \"no_speech_prob\": 0.3598945140838623,\n",
       "      \"seek\": 0,\n",
       "      \"start\": 9.84,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Juan is responsible for knowing everything in your company, but first things first, our\",\n",
       "      \"tokens\": [\n",
       "        17064,\n",
       "        307,\n",
       "        6250,\n",
       "        337,\n",
       "        5276,\n",
       "        1203,\n",
       "        294,\n",
       "        428,\n",
       "        2237,\n",
       "        11,\n",
       "        457,\n",
       "        700,\n",
       "        721,\n",
       "        700,\n",
       "        11,\n",
       "        527\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.34817066899052374,\n",
       "      \"compression_ratio\": 1.4768518518518519,\n",
       "      \"end\": 23.400000000000002,\n",
       "      \"id\": 3,\n",
       "      \"no_speech_prob\": 0.3598945140838623,\n",
       "      \"seek\": 0,\n",
       "      \"start\": 16.56,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather\",\n",
       "      \"tokens\": [\n",
       "        1469,\n",
       "        307,\n",
       "        490,\n",
       "        31571,\n",
       "        11,\n",
       "        376,\n",
       "        2641,\n",
       "        27924,\n",
       "        11,\n",
       "        26501,\n",
       "        11,\n",
       "        293,\n",
       "        286,\n",
       "        5665,\n",
       "        1621,\n",
       "        294,\n",
       "        7244,\n",
       "        11,\n",
       "        293,\n",
       "        321,\n",
       "        439,\n",
       "        5448\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.34817066899052374,\n",
       "      \"compression_ratio\": 1.4768518518518519,\n",
       "      \"end\": 26.0,\n",
       "      \"id\": 4,\n",
       "      \"no_speech_prob\": 0.3598945140838623,\n",
       "      \"seek\": 0,\n",
       "      \"start\": 23.400000000000002,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" together to solve one simple problem.\",\n",
       "      \"tokens\": [\n",
       "        1214,\n",
       "        281,\n",
       "        5039,\n",
       "        472,\n",
       "        2199,\n",
       "        1154,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.38286988453198506,\n",
       "      \"compression_ratio\": 1.6929824561403508,\n",
       "      \"end\": 30.92,\n",
       "      \"id\": 5,\n",
       "      \"no_speech_prob\": 9.271639282815158e-05,\n",
       "      \"seek\": 2600,\n",
       "      \"start\": 26.0,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" The problem is that we are really spoiled by quality of Google search.\",\n",
       "      \"tokens\": [\n",
       "        440,\n",
       "        1154,\n",
       "        307,\n",
       "        300,\n",
       "        321,\n",
       "        366,\n",
       "        534,\n",
       "        32439,\n",
       "        538,\n",
       "        3125,\n",
       "        295,\n",
       "        3329,\n",
       "        3164,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.38286988453198506,\n",
       "      \"compression_ratio\": 1.6929824561403508,\n",
       "      \"end\": 37.68,\n",
       "      \"id\": 6,\n",
       "      \"no_speech_prob\": 9.271639282815158e-05,\n",
       "      \"seek\": 2600,\n",
       "      \"start\": 30.92,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So spoiled that every time we need to find something in our corporate systems, we suffer,\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        32439,\n",
       "        300,\n",
       "        633,\n",
       "        565,\n",
       "        321,\n",
       "        643,\n",
       "        281,\n",
       "        915,\n",
       "        746,\n",
       "        294,\n",
       "        527,\n",
       "        10896,\n",
       "        3652,\n",
       "        11,\n",
       "        321,\n",
       "        9753,\n",
       "        11\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.38286988453198506,\n",
       "      \"compression_ratio\": 1.6929824561403508,\n",
       "      \"end\": 43.28,\n",
       "      \"id\": 7,\n",
       "      \"no_speech_prob\": 9.271639282815158e-05,\n",
       "      \"seek\": 2600,\n",
       "      \"start\": 37.68,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" because they usually use the keyword search, the old style, not the modern one that Google,\",\n",
       "      \"tokens\": [\n",
       "        570,\n",
       "        436,\n",
       "        2673,\n",
       "        764,\n",
       "        264,\n",
       "        20428,\n",
       "        3164,\n",
       "        11,\n",
       "        264,\n",
       "        1331,\n",
       "        3758,\n",
       "        11,\n",
       "        406,\n",
       "        264,\n",
       "        4363,\n",
       "        472,\n",
       "        300,\n",
       "        3329,\n",
       "        11\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.38286988453198506,\n",
       "      \"compression_ratio\": 1.6929824561403508,\n",
       "      \"end\": 47.86,\n",
       "      \"id\": 8,\n",
       "      \"no_speech_prob\": 9.271639282815158e-05,\n",
       "      \"seek\": 2600,\n",
       "      \"start\": 43.28,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" not modern one, semantic, that Google use.\",\n",
       "      \"tokens\": [\n",
       "        406,\n",
       "        4363,\n",
       "        472,\n",
       "        11,\n",
       "        47982,\n",
       "        11,\n",
       "        300,\n",
       "        3329,\n",
       "        764,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.38286988453198506,\n",
       "      \"compression_ratio\": 1.6929824561403508,\n",
       "      \"end\": 55.44,\n",
       "      \"id\": 9,\n",
       "      \"no_speech_prob\": 9.271639282815158e-05,\n",
       "      \"seek\": 2600,\n",
       "      \"start\": 47.86,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast,\",\n",
       "      \"tokens\": [\n",
       "        583,\n",
       "        264,\n",
       "        1154,\n",
       "        307,\n",
       "        709,\n",
       "        3801,\n",
       "        11,\n",
       "        300,\n",
       "        257,\n",
       "        688,\n",
       "        295,\n",
       "        3601,\n",
       "        307,\n",
       "        10596,\n",
       "        294,\n",
       "        264,\n",
       "        8287,\n",
       "        3670,\n",
       "        3734,\n",
       "        11\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.40586188260246725,\n",
       "      \"compression_ratio\": 1.4568527918781726,\n",
       "      \"end\": 58.04,\n",
       "      \"id\": 10,\n",
       "      \"no_speech_prob\": 6.450530781876296e-05,\n",
       "      \"seek\": 5544,\n",
       "      \"start\": 55.44,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" like you are looking now.\",\n",
       "      \"tokens\": [\n",
       "        411,\n",
       "        291,\n",
       "        366,\n",
       "        1237,\n",
       "        586,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.40586188260246725,\n",
       "      \"compression_ratio\": 1.4568527918781726,\n",
       "      \"end\": 65.32,\n",
       "      \"id\": 11,\n",
       "      \"no_speech_prob\": 6.450530781876296e-05,\n",
       "      \"seek\": 5544,\n",
       "      \"start\": 58.04,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Imagine that all video presentation of business processes, systems, solutions, or ever greater\",\n",
       "      \"tokens\": [\n",
       "        11739,\n",
       "        300,\n",
       "        439,\n",
       "        960,\n",
       "        5860,\n",
       "        295,\n",
       "        1606,\n",
       "        7555,\n",
       "        11,\n",
       "        3652,\n",
       "        11,\n",
       "        6547,\n",
       "        11,\n",
       "        420,\n",
       "        1562,\n",
       "        5044\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.40586188260246725,\n",
       "      \"compression_ratio\": 1.4568527918781726,\n",
       "      \"end\": 72.47999999999999,\n",
       "      \"id\": 12,\n",
       "      \"no_speech_prob\": 6.450530781876296e-05,\n",
       "      \"seek\": 5544,\n",
       "      \"start\": 65.32,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" ideas in your company will be converted into the Wikipedia pages and will be searchable\",\n",
       "      \"tokens\": [\n",
       "        3487,\n",
       "        294,\n",
       "        428,\n",
       "        2237,\n",
       "        486,\n",
       "        312,\n",
       "        16424,\n",
       "        666,\n",
       "        264,\n",
       "        28999,\n",
       "        7183,\n",
       "        293,\n",
       "        486,\n",
       "        312,\n",
       "        3164,\n",
       "        712\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.40586188260246725,\n",
       "      \"compression_ratio\": 1.4568527918781726,\n",
       "      \"end\": 75.72,\n",
       "      \"id\": 13,\n",
       "      \"no_speech_prob\": 6.450530781876296e-05,\n",
       "      \"seek\": 5544,\n",
       "      \"start\": 72.47999999999999,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" by modern search engines.\",\n",
       "      \"tokens\": [\n",
       "        538,\n",
       "        4363,\n",
       "        3164,\n",
       "        12982,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.40586188260246725,\n",
       "      \"compression_ratio\": 1.4568527918781726,\n",
       "      \"end\": 80.47999999999999,\n",
       "      \"id\": 14,\n",
       "      \"no_speech_prob\": 6.450530781876296e-05,\n",
       "      \"seek\": 5544,\n",
       "      \"start\": 75.72,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" And that's exactly our, what Juan Holloway is doing.\",\n",
       "      \"tokens\": [\n",
       "        400,\n",
       "        300,\n",
       "        311,\n",
       "        2293,\n",
       "        527,\n",
       "        11,\n",
       "        437,\n",
       "        17064,\n",
       "        46731,\n",
       "        320,\n",
       "        307,\n",
       "        884,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.2952813662015475,\n",
       "      \"compression_ratio\": 1.4941176470588236,\n",
       "      \"end\": 85.76,\n",
       "      \"id\": 15,\n",
       "      \"no_speech_prob\": 2.4228347683674656e-06,\n",
       "      \"seek\": 8048,\n",
       "      \"start\": 80.48,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Let me show you.\",\n",
       "      \"tokens\": [\n",
       "        961,\n",
       "        385,\n",
       "        855,\n",
       "        291,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.2952813662015475,\n",
       "      \"compression_ratio\": 1.4941176470588236,\n",
       "      \"end\": 94.46000000000001,\n",
       "      \"id\": 16,\n",
       "      \"no_speech_prob\": 2.4228347683674656e-06,\n",
       "      \"seek\": 8048,\n",
       "      \"start\": 85.76,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" To make a demo, we convert some results from previous hackathons into our Wikipedia.\",\n",
       "      \"tokens\": [\n",
       "        1407,\n",
       "        652,\n",
       "        257,\n",
       "        10723,\n",
       "        11,\n",
       "        321,\n",
       "        7620,\n",
       "        512,\n",
       "        3542,\n",
       "        490,\n",
       "        3894,\n",
       "        10339,\n",
       "        998,\n",
       "        892,\n",
       "        666,\n",
       "        527,\n",
       "        28999,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.2952813662015475,\n",
       "      \"compression_ratio\": 1.4941176470588236,\n",
       "      \"end\": 102.36,\n",
       "      \"id\": 17,\n",
       "      \"no_speech_prob\": 2.4228347683674656e-06,\n",
       "      \"seek\": 8048,\n",
       "      \"start\": 94.46000000000001,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        309,\n",
       "        311,\n",
       "        13735,\n",
       "        28999,\n",
       "        11,\n",
       "        28999,\n",
       "        300,\n",
       "        2089,\n",
       "        291,\n",
       "        8129,\n",
       "        7183,\n",
       "        11,\n",
       "        909,\n",
       "        777,\n",
       "        2306,\n",
       "        11,\n",
       "        362,\n",
       "        512\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.2952813662015475,\n",
       "      \"compression_ratio\": 1.4941176470588236,\n",
       "      \"end\": 108.60000000000001,\n",
       "      \"id\": 18,\n",
       "      \"no_speech_prob\": 2.4228347683674656e-06,\n",
       "      \"seek\": 8048,\n",
       "      \"start\": 102.36,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" structure here, and whatever, just editor for the small pages.\",\n",
       "      \"tokens\": [\n",
       "        3877,\n",
       "        510,\n",
       "        11,\n",
       "        293,\n",
       "        2035,\n",
       "        11,\n",
       "        445,\n",
       "        9839,\n",
       "        337,\n",
       "        264,\n",
       "        1359,\n",
       "        7183,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.24560068731438622,\n",
       "      \"compression_ratio\": 1.513089005235602,\n",
       "      \"end\": 117.67999999999999,\n",
       "      \"id\": 19,\n",
       "      \"no_speech_prob\": 6.537604804179864e-06,\n",
       "      \"seek\": 10860,\n",
       "      \"start\": 108.6,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So we converted demos of a few teams into this Wikipedia format using Whisper and some\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        321,\n",
       "        16424,\n",
       "        33788,\n",
       "        295,\n",
       "        257,\n",
       "        1326,\n",
       "        5491,\n",
       "        666,\n",
       "        341,\n",
       "        28999,\n",
       "        7877,\n",
       "        1228,\n",
       "        41132,\n",
       "        610,\n",
       "        293,\n",
       "        512\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.24560068731438622,\n",
       "      \"compression_ratio\": 1.513089005235602,\n",
       "      \"end\": 121.67999999999999,\n",
       "      \"id\": 20,\n",
       "      \"no_speech_prob\": 6.537604804179864e-06,\n",
       "      \"seek\": 10860,\n",
       "      \"start\": 117.67999999999999,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" magic from GPT to make them much more readable.\",\n",
       "      \"tokens\": [\n",
       "        5585,\n",
       "        490,\n",
       "        26039,\n",
       "        51,\n",
       "        281,\n",
       "        652,\n",
       "        552,\n",
       "        709,\n",
       "        544,\n",
       "        49857,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.24560068731438622,\n",
       "      \"compression_ratio\": 1.513089005235602,\n",
       "      \"end\": 129.07999999999998,\n",
       "      \"id\": 21,\n",
       "      \"no_speech_prob\": 6.537604804179864e-06,\n",
       "      \"seek\": 10860,\n",
       "      \"start\": 121.67999999999999,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" But what is more important, we allow them to be searchable by semantic search.\",\n",
       "      \"tokens\": [\n",
       "        583,\n",
       "        437,\n",
       "        307,\n",
       "        544,\n",
       "        1021,\n",
       "        11,\n",
       "        321,\n",
       "        2089,\n",
       "        552,\n",
       "        281,\n",
       "        312,\n",
       "        3164,\n",
       "        712,\n",
       "        538,\n",
       "        47982,\n",
       "        3164,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.24560068731438622,\n",
       "      \"compression_ratio\": 1.513089005235602,\n",
       "      \"end\": 134.32,\n",
       "      \"id\": 22,\n",
       "      \"no_speech_prob\": 6.537604804179864e-06,\n",
       "      \"seek\": 10860,\n",
       "      \"start\": 129.07999999999998,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So we use a multilingual semantic search, and I will show you how it works.\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        321,\n",
       "        764,\n",
       "        257,\n",
       "        2120,\n",
       "        38219,\n",
       "        47982,\n",
       "        3164,\n",
       "        11,\n",
       "        293,\n",
       "        286,\n",
       "        486,\n",
       "        855,\n",
       "        291,\n",
       "        577,\n",
       "        309,\n",
       "        1985,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.560177729679988,\n",
       "      \"compression_ratio\": 1.2242990654205608,\n",
       "      \"end\": 141.2,\n",
       "      \"id\": 23,\n",
       "      \"no_speech_prob\": 9.144811338046566e-05,\n",
       "      \"seek\": 13432,\n",
       "      \"start\": 134.32,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" We have some project, Ask Quran, let's try to formulate it in Russian, like a questions\",\n",
       "      \"tokens\": [\n",
       "        492,\n",
       "        362,\n",
       "        512,\n",
       "        1716,\n",
       "        11,\n",
       "        12320,\n",
       "        19375,\n",
       "        11,\n",
       "        718,\n",
       "        311,\n",
       "        853,\n",
       "        281,\n",
       "        47881,\n",
       "        309,\n",
       "        294,\n",
       "        7220,\n",
       "        11,\n",
       "        411,\n",
       "        257,\n",
       "        1651\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.560177729679988,\n",
       "      \"compression_ratio\": 1.2242990654205608,\n",
       "      \"end\": 144.92,\n",
       "      \"id\": 24,\n",
       "      \"no_speech_prob\": 9.144811338046566e-05,\n",
       "      \"seek\": 13432,\n",
       "      \"start\": 141.2,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" about God.\",\n",
       "      \"tokens\": [\n",
       "        466,\n",
       "        1265,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.560177729679988,\n",
       "      \"compression_ratio\": 1.2242990654205608,\n",
       "      \"end\": 155.92,\n",
       "      \"id\": 25,\n",
       "      \"no_speech_prob\": 9.144811338046566e-05,\n",
       "      \"seek\": 13432,\n",
       "      \"start\": 144.92,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Yeah, we have Ask Quran project.\",\n",
       "      \"tokens\": [\n",
       "        865,\n",
       "        11,\n",
       "        321,\n",
       "        362,\n",
       "        12320,\n",
       "        19375,\n",
       "        1716,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.544442892074585,\n",
       "      \"compression_ratio\": 1.2300884955752212,\n",
       "      \"end\": 164.92,\n",
       "      \"id\": 26,\n",
       "      \"no_speech_prob\": 1.7054564978025155e-06,\n",
       "      \"seek\": 15592,\n",
       "      \"start\": 155.92,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" It's more delay on loading.\",\n",
       "      \"tokens\": [\n",
       "        467,\n",
       "        311,\n",
       "        544,\n",
       "        8577,\n",
       "        322,\n",
       "        15114,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.544442892074585,\n",
       "      \"compression_ratio\": 1.2300884955752212,\n",
       "      \"end\": 178.67999999999998,\n",
       "      \"id\": 27,\n",
       "      \"no_speech_prob\": 1.7054564978025155e-06,\n",
       "      \"seek\": 15592,\n",
       "      \"start\": 164.92,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" I will skip a bit.\",\n",
       "      \"tokens\": [\n",
       "        286,\n",
       "        486,\n",
       "        10023,\n",
       "        257,\n",
       "        857,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.544442892074585,\n",
       "      \"compression_ratio\": 1.2300884955752212,\n",
       "      \"end\": 185.35999999999999,\n",
       "      \"id\": 28,\n",
       "      \"no_speech_prob\": 1.7054564978025155e-06,\n",
       "      \"seek\": 15592,\n",
       "      \"start\": 178.67999999999998,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Seems like it's quite a good match about questions about God in Russian to the English text.\",\n",
       "      \"tokens\": [\n",
       "        22524,\n",
       "        411,\n",
       "        309,\n",
       "        311,\n",
       "        1596,\n",
       "        257,\n",
       "        665,\n",
       "        2995,\n",
       "        466,\n",
       "        1651,\n",
       "        466,\n",
       "        1265,\n",
       "        294,\n",
       "        7220,\n",
       "        281,\n",
       "        264,\n",
       "        3669,\n",
       "        2487,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.35132277453387223,\n",
       "      \"compression_ratio\": 1.5245901639344261,\n",
       "      \"end\": 193.70000000000002,\n",
       "      \"id\": 29,\n",
       "      \"no_speech_prob\": 3.5311561077833176e-05,\n",
       "      \"seek\": 18536,\n",
       "      \"start\": 185.36,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So let me show you how our system, our Wikipedias works with video.\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        718,\n",
       "        385,\n",
       "        855,\n",
       "        291,\n",
       "        577,\n",
       "        527,\n",
       "        1185,\n",
       "        11,\n",
       "        527,\n",
       "        23377,\n",
       "        647,\n",
       "        292,\n",
       "        4609,\n",
       "        1985,\n",
       "        365,\n",
       "        960,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.35132277453387223,\n",
       "      \"compression_ratio\": 1.5245901639344261,\n",
       "      \"end\": 205.64000000000001,\n",
       "      \"id\": 30,\n",
       "      \"no_speech_prob\": 3.5311561077833176e-05,\n",
       "      \"seek\": 18536,\n",
       "      \"start\": 193.70000000000002,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So when video is playing, we highlight the part of the video, or you can jump to the\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        562,\n",
       "        960,\n",
       "        307,\n",
       "        2433,\n",
       "        11,\n",
       "        321,\n",
       "        5078,\n",
       "        264,\n",
       "        644,\n",
       "        295,\n",
       "        264,\n",
       "        960,\n",
       "        11,\n",
       "        420,\n",
       "        291,\n",
       "        393,\n",
       "        3012,\n",
       "        281,\n",
       "        264\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.35132277453387223,\n",
       "      \"compression_ratio\": 1.5245901639344261,\n",
       "      \"end\": 212.72000000000003,\n",
       "      \"id\": 31,\n",
       "      \"no_speech_prob\": 3.5311561077833176e-05,\n",
       "      \"seek\": 18536,\n",
       "      \"start\": 205.64000000000001,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" part of the video from the video.\",\n",
       "      \"tokens\": [\n",
       "        644,\n",
       "        295,\n",
       "        264,\n",
       "        960,\n",
       "        490,\n",
       "        264,\n",
       "        960,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.28872651639192,\n",
       "      \"compression_ratio\": 1.6127450980392157,\n",
       "      \"end\": 219.68,\n",
       "      \"id\": 32,\n",
       "      \"no_speech_prob\": 3.269177977927029e-05,\n",
       "      \"seek\": 21272,\n",
       "      \"start\": 212.72,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" We could jump to the timestamp in the video, clicking to the text, or you can switch the\",\n",
       "      \"tokens\": [\n",
       "        492,\n",
       "        727,\n",
       "        3012,\n",
       "        281,\n",
       "        264,\n",
       "        49108,\n",
       "        1215,\n",
       "        294,\n",
       "        264,\n",
       "        960,\n",
       "        11,\n",
       "        9697,\n",
       "        281,\n",
       "        264,\n",
       "        2487,\n",
       "        11,\n",
       "        420,\n",
       "        291,\n",
       "        393,\n",
       "        3679,\n",
       "        264\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.28872651639192,\n",
       "      \"compression_ratio\": 1.6127450980392157,\n",
       "      \"end\": 223.28,\n",
       "      \"id\": 33,\n",
       "      \"no_speech_prob\": 3.269177977927029e-05,\n",
       "      \"seek\": 21272,\n",
       "      \"start\": 219.68,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" video part and see which part of the text it is.\",\n",
       "      \"tokens\": [\n",
       "        960,\n",
       "        644,\n",
       "        293,\n",
       "        536,\n",
       "        597,\n",
       "        644,\n",
       "        295,\n",
       "        264,\n",
       "        2487,\n",
       "        309,\n",
       "        307,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.28872651639192,\n",
       "      \"compression_ratio\": 1.6127450980392157,\n",
       "      \"end\": 231.36,\n",
       "      \"id\": 34,\n",
       "      \"no_speech_prob\": 3.269177977927029e-05,\n",
       "      \"seek\": 21272,\n",
       "      \"start\": 223.28,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search.\",\n",
       "      \"tokens\": [\n",
       "        876,\n",
       "        11,\n",
       "        309,\n",
       "        311,\n",
       "        257,\n",
       "        857,\n",
       "        484,\n",
       "        295,\n",
       "        11923,\n",
       "        295,\n",
       "        2190,\n",
       "        10339,\n",
       "        18660,\n",
       "        11,\n",
       "        293,\n",
       "        321,\n",
       "        366,\n",
       "        1417,\n",
       "        466,\n",
       "        47982,\n",
       "        3164,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.28872651639192,\n",
       "      \"compression_ratio\": 1.6127450980392157,\n",
       "      \"end\": 238.88,\n",
       "      \"id\": 35,\n",
       "      \"no_speech_prob\": 3.269177977927029e-05,\n",
       "      \"seek\": 21272,\n",
       "      \"start\": 231.36,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So I will try to write in Spanish, not as personales, which is personal nouns, and see\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        286,\n",
       "        486,\n",
       "        853,\n",
       "        281,\n",
       "        2464,\n",
       "        294,\n",
       "        8058,\n",
       "        11,\n",
       "        406,\n",
       "        382,\n",
       "        2973,\n",
       "        279,\n",
       "        11,\n",
       "        597,\n",
       "        307,\n",
       "        2973,\n",
       "        48184,\n",
       "        11,\n",
       "        293,\n",
       "        536\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.28872651639192,\n",
       "      \"compression_ratio\": 1.6127450980392157,\n",
       "      \"end\": 239.88,\n",
       "      \"id\": 36,\n",
       "      \"no_speech_prob\": 3.269177977927029e-05,\n",
       "      \"seek\": 21272,\n",
       "      \"start\": 238.88,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" what we get.\",\n",
       "      \"tokens\": [\n",
       "        437,\n",
       "        321,\n",
       "        483,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3481828717217929,\n",
       "      \"compression_ratio\": 1.5146198830409356,\n",
       "      \"end\": 247.79999999999998,\n",
       "      \"id\": 37,\n",
       "      \"no_speech_prob\": 1.3499182387022302e-05,\n",
       "      \"seek\": 23988,\n",
       "      \"start\": 239.88,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So we have a daily journaling app presentation.\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        321,\n",
       "        362,\n",
       "        257,\n",
       "        5212,\n",
       "        17598,\n",
       "        4270,\n",
       "        724,\n",
       "        5860,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3481828717217929,\n",
       "      \"compression_ratio\": 1.5146198830409356,\n",
       "      \"end\": 251.07999999999998,\n",
       "      \"id\": 38,\n",
       "      \"no_speech_prob\": 1.3499182387022302e-05,\n",
       "      \"seek\": 23988,\n",
       "      \"start\": 247.79999999999998,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" To speed up, I will click here.\",\n",
       "      \"tokens\": [\n",
       "        1407,\n",
       "        3073,\n",
       "        493,\n",
       "        11,\n",
       "        286,\n",
       "        486,\n",
       "        2052,\n",
       "        510,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3481828717217929,\n",
       "      \"compression_ratio\": 1.5146198830409356,\n",
       "      \"end\": 258.32,\n",
       "      \"id\": 39,\n",
       "      \"no_speech_prob\": 1.3499182387022302e-05,\n",
       "      \"seek\": 23988,\n",
       "      \"start\": 251.07999999999998,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" In conversational daily journaling experience, where we hope to enable users to write your\",\n",
       "      \"tokens\": [\n",
       "        682,\n",
       "        2615,\n",
       "        1478,\n",
       "        5212,\n",
       "        17598,\n",
       "        4270,\n",
       "        1752,\n",
       "        11,\n",
       "        689,\n",
       "        321,\n",
       "        1454,\n",
       "        281,\n",
       "        9528,\n",
       "        5022,\n",
       "        281,\n",
       "        2464,\n",
       "        428\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3481828717217929,\n",
       "      \"compression_ratio\": 1.5146198830409356,\n",
       "      \"end\": 263.36,\n",
       "      \"id\": 40,\n",
       "      \"no_speech_prob\": 1.3499182387022302e-05,\n",
       "      \"seek\": 23988,\n",
       "      \"start\": 258.32,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" way to a better day.\",\n",
       "      \"tokens\": [\n",
       "        636,\n",
       "        281,\n",
       "        257,\n",
       "        1101,\n",
       "        786,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3481828717217929,\n",
       "      \"compression_ratio\": 1.5146198830409356,\n",
       "      \"end\": 267.88,\n",
       "      \"id\": 41,\n",
       "      \"no_speech_prob\": 1.3499182387022302e-05,\n",
       "      \"seek\": 23988,\n",
       "      \"start\": 263.36,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" That seems quite a good match for the questions, not as personales.\",\n",
       "      \"tokens\": [\n",
       "        663,\n",
       "        2544,\n",
       "        1596,\n",
       "        257,\n",
       "        665,\n",
       "        2995,\n",
       "        337,\n",
       "        264,\n",
       "        1651,\n",
       "        11,\n",
       "        406,\n",
       "        382,\n",
       "        2973,\n",
       "        279,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.33751848581674937,\n",
       "      \"compression_ratio\": 1.4682926829268292,\n",
       "      \"end\": 273.71999999999997,\n",
       "      \"id\": 42,\n",
       "      \"no_speech_prob\": 3.6826440918957815e-05,\n",
       "      \"seek\": 26788,\n",
       "      \"start\": 267.88,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Well, enough for the glamour, back to slides.\",\n",
       "      \"tokens\": [\n",
       "        1042,\n",
       "        11,\n",
       "        1547,\n",
       "        337,\n",
       "        264,\n",
       "        28133,\n",
       "        396,\n",
       "        11,\n",
       "        646,\n",
       "        281,\n",
       "        9788,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.33751848581674937,\n",
       "      \"compression_ratio\": 1.4682926829268292,\n",
       "      \"end\": 280.0,\n",
       "      \"id\": 43,\n",
       "      \"no_speech_prob\": 3.6826440918957815e-05,\n",
       "      \"seek\": 26788,\n",
       "      \"start\": 273.71999999999997,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Before jumping into the technical details, I would like to say that this knowledge management\",\n",
       "      \"tokens\": [\n",
       "        4546,\n",
       "        11233,\n",
       "        666,\n",
       "        264,\n",
       "        6191,\n",
       "        4365,\n",
       "        11,\n",
       "        286,\n",
       "        576,\n",
       "        411,\n",
       "        281,\n",
       "        584,\n",
       "        300,\n",
       "        341,\n",
       "        3601,\n",
       "        4592\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.33751848581674937,\n",
       "      \"compression_ratio\": 1.4682926829268292,\n",
       "      \"end\": 282.28,\n",
       "      \"id\": 44,\n",
       "      \"no_speech_prob\": 3.6826440918957815e-05,\n",
       "      \"seek\": 26788,\n",
       "      \"start\": 280.0,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" market is huge.\",\n",
       "      \"tokens\": [\n",
       "        2142,\n",
       "        307,\n",
       "        2603,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.33751848581674937,\n",
       "      \"compression_ratio\": 1.4682926829268292,\n",
       "      \"end\": 291.44,\n",
       "      \"id\": 45,\n",
       "      \"no_speech_prob\": 3.6826440918957815e-05,\n",
       "      \"seek\": 26788,\n",
       "      \"start\": 282.28,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two\",\n",
       "      \"tokens\": [\n",
       "        467,\n",
       "        311,\n",
       "        12690,\n",
       "        294,\n",
       "        6779,\n",
       "        295,\n",
       "        17375,\n",
       "        295,\n",
       "        3808,\n",
       "        257,\n",
       "        1064,\n",
       "        11,\n",
       "        293,\n",
       "        309,\n",
       "        307,\n",
       "        4194,\n",
       "        1596,\n",
       "        2370,\n",
       "        337,\n",
       "        732\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.33751848581674937,\n",
       "      \"compression_ratio\": 1.4682926829268292,\n",
       "      \"end\": 296.12,\n",
       "      \"id\": 46,\n",
       "      \"no_speech_prob\": 3.6826440918957815e-05,\n",
       "      \"seek\": 26788,\n",
       "      \"start\": 291.44,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" digits figures, depending on which report you read.\",\n",
       "      \"tokens\": [\n",
       "        27011,\n",
       "        9624,\n",
       "        11,\n",
       "        5413,\n",
       "        322,\n",
       "        597,\n",
       "        2275,\n",
       "        291,\n",
       "        1401,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3493703206380208,\n",
       "      \"compression_ratio\": 1.446236559139785,\n",
       "      \"end\": 306.32,\n",
       "      \"id\": 47,\n",
       "      \"no_speech_prob\": 4.980812445865013e-05,\n",
       "      \"seek\": 29612,\n",
       "      \"start\": 296.12,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" And it's quite a good trend to disrupt this market by new AI solutions or AI technologies,\",\n",
       "      \"tokens\": [\n",
       "        400,\n",
       "        309,\n",
       "        311,\n",
       "        1596,\n",
       "        257,\n",
       "        665,\n",
       "        6028,\n",
       "        281,\n",
       "        14124,\n",
       "        341,\n",
       "        2142,\n",
       "        538,\n",
       "        777,\n",
       "        7318,\n",
       "        6547,\n",
       "        420,\n",
       "        7318,\n",
       "        7943,\n",
       "        11\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3493703206380208,\n",
       "      \"compression_ratio\": 1.446236559139785,\n",
       "      \"end\": 308.52,\n",
       "      \"id\": 48,\n",
       "      \"no_speech_prob\": 4.980812445865013e-05,\n",
       "      \"seek\": 29612,\n",
       "      \"start\": 306.32,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" like our IHANA.\",\n",
       "      \"tokens\": [\n",
       "        411,\n",
       "        527,\n",
       "        286,\n",
       "        39,\n",
       "        21429,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3493703206380208,\n",
       "      \"compression_ratio\": 1.446236559139785,\n",
       "      \"end\": 315.68,\n",
       "      \"id\": 49,\n",
       "      \"no_speech_prob\": 4.980812445865013e-05,\n",
       "      \"seek\": 29612,\n",
       "      \"start\": 308.52,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So that could not be just a fancy toy with technology, but it could be a part of the\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        300,\n",
       "        727,\n",
       "        406,\n",
       "        312,\n",
       "        445,\n",
       "        257,\n",
       "        10247,\n",
       "        12058,\n",
       "        365,\n",
       "        2899,\n",
       "        11,\n",
       "        457,\n",
       "        309,\n",
       "        727,\n",
       "        312,\n",
       "        257,\n",
       "        644,\n",
       "        295,\n",
       "        264\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3493703206380208,\n",
       "      \"compression_ratio\": 1.446236559139785,\n",
       "      \"end\": 322.0,\n",
       "      \"id\": 50,\n",
       "      \"no_speech_prob\": 4.980812445865013e-05,\n",
       "      \"seek\": 29612,\n",
       "      \"start\": 315.68,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" business because this solution is applicable in every enterprise environment.\",\n",
       "      \"tokens\": [\n",
       "        1606,\n",
       "        570,\n",
       "        341,\n",
       "        3827,\n",
       "        307,\n",
       "        21142,\n",
       "        294,\n",
       "        633,\n",
       "        14132,\n",
       "        2823,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3417972067128057,\n",
       "      \"compression_ratio\": 1.4893617021276595,\n",
       "      \"end\": 331.72,\n",
       "      \"id\": 51,\n",
       "      \"no_speech_prob\": 6.900524749653414e-05,\n",
       "      \"seek\": 32200,\n",
       "      \"start\": 322.0,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Well, then it could help the people who cannot listen to or watch videos to read information.\",\n",
       "      \"tokens\": [\n",
       "        1042,\n",
       "        11,\n",
       "        550,\n",
       "        309,\n",
       "        727,\n",
       "        854,\n",
       "        264,\n",
       "        561,\n",
       "        567,\n",
       "        2644,\n",
       "        2140,\n",
       "        281,\n",
       "        420,\n",
       "        1159,\n",
       "        2145,\n",
       "        281,\n",
       "        1401,\n",
       "        1589,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3417972067128057,\n",
       "      \"compression_ratio\": 1.4893617021276595,\n",
       "      \"end\": 340.4,\n",
       "      \"id\": 52,\n",
       "      \"no_speech_prob\": 6.900524749653414e-05,\n",
       "      \"seek\": 32200,\n",
       "      \"start\": 331.72,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" And I would say you can help busy managers to spend some time reading instead of watching\",\n",
       "      \"tokens\": [\n",
       "        400,\n",
       "        286,\n",
       "        576,\n",
       "        584,\n",
       "        291,\n",
       "        393,\n",
       "        854,\n",
       "        5856,\n",
       "        14084,\n",
       "        281,\n",
       "        3496,\n",
       "        512,\n",
       "        565,\n",
       "        3760,\n",
       "        2602,\n",
       "        295,\n",
       "        1976\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3417972067128057,\n",
       "      \"compression_ratio\": 1.4893617021276595,\n",
       "      \"end\": 341.8,\n",
       "      \"id\": 53,\n",
       "      \"no_speech_prob\": 6.900524749653414e-05,\n",
       "      \"seek\": 32200,\n",
       "      \"start\": 340.4,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" videos.\",\n",
       "      \"tokens\": [\n",
       "        2145,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3417972067128057,\n",
       "      \"compression_ratio\": 1.4893617021276595,\n",
       "      \"end\": 349.32,\n",
       "      \"id\": 54,\n",
       "      \"no_speech_prob\": 6.900524749653414e-05,\n",
       "      \"seek\": 32200,\n",
       "      \"start\": 341.8,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the\",\n",
       "      \"tokens\": [\n",
       "        1743,\n",
       "        510,\n",
       "        11,\n",
       "        321,\n",
       "        393,\n",
       "        3079,\n",
       "        294,\n",
       "        2177,\n",
       "        455,\n",
       "        44990,\n",
       "        7318,\n",
       "        570,\n",
       "        321,\n",
       "        362,\n",
       "        257,\n",
       "        688,\n",
       "        295,\n",
       "        3601,\n",
       "        8287,\n",
       "        281,\n",
       "        264\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.36406986530010516,\n",
       "      \"compression_ratio\": 1.349112426035503,\n",
       "      \"end\": 355.56,\n",
       "      \"id\": 55,\n",
       "      \"no_speech_prob\": 3.717320942087099e-05,\n",
       "      \"seek\": 34932,\n",
       "      \"start\": 349.32,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" webcasts, but they are not searchable, so I guess it's not accessible.\",\n",
       "      \"tokens\": [\n",
       "        3670,\n",
       "        3734,\n",
       "        82,\n",
       "        11,\n",
       "        457,\n",
       "        436,\n",
       "        366,\n",
       "        406,\n",
       "        3164,\n",
       "        712,\n",
       "        11,\n",
       "        370,\n",
       "        286,\n",
       "        2041,\n",
       "        309,\n",
       "        311,\n",
       "        406,\n",
       "        9515,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.36406986530010516,\n",
       "      \"compression_ratio\": 1.349112426035503,\n",
       "      \"end\": 360.04,\n",
       "      \"id\": 56,\n",
       "      \"no_speech_prob\": 3.717320942087099e-05,\n",
       "      \"seek\": 34932,\n",
       "      \"start\": 355.56,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Well, how it was done.\",\n",
       "      \"tokens\": [\n",
       "        1042,\n",
       "        11,\n",
       "        577,\n",
       "        309,\n",
       "        390,\n",
       "        1096,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.36406986530010516,\n",
       "      \"compression_ratio\": 1.349112426035503,\n",
       "      \"end\": 363.44,\n",
       "      \"id\": 57,\n",
       "      \"no_speech_prob\": 3.717320942087099e-05,\n",
       "      \"seek\": 34932,\n",
       "      \"start\": 360.04,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" As a base, we use an open source knowledge management system, Wikijs.\",\n",
       "      \"tokens\": [\n",
       "        1018,\n",
       "        257,\n",
       "        3096,\n",
       "        11,\n",
       "        321,\n",
       "        764,\n",
       "        364,\n",
       "        1269,\n",
       "        4009,\n",
       "        3601,\n",
       "        4592,\n",
       "        1185,\n",
       "        11,\n",
       "        23377,\n",
       "        1718,\n",
       "        82,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.36406986530010516,\n",
       "      \"compression_ratio\": 1.349112426035503,\n",
       "      \"end\": 370.03999999999996,\n",
       "      \"id\": 58,\n",
       "      \"no_speech_prob\": 3.717320942087099e-05,\n",
       "      \"seek\": 34932,\n",
       "      \"start\": 363.44,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" It's quite commercially used and popular, written in JavaScript.\",\n",
       "      \"tokens\": [\n",
       "        467,\n",
       "        311,\n",
       "        1596,\n",
       "        41751,\n",
       "        1143,\n",
       "        293,\n",
       "        3743,\n",
       "        11,\n",
       "        3720,\n",
       "        294,\n",
       "        15778,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.397910246977935,\n",
       "      \"compression_ratio\": 1.6065573770491803,\n",
       "      \"end\": 379.52000000000004,\n",
       "      \"id\": 59,\n",
       "      \"no_speech_prob\": 2.8154952815384604e-05,\n",
       "      \"seek\": 37004,\n",
       "      \"start\": 370.04,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" It already supports some amount of search engines that you can set up and it will use\",\n",
       "      \"tokens\": [\n",
       "        467,\n",
       "        1217,\n",
       "        9346,\n",
       "        512,\n",
       "        2372,\n",
       "        295,\n",
       "        3164,\n",
       "        12982,\n",
       "        300,\n",
       "        291,\n",
       "        393,\n",
       "        992,\n",
       "        493,\n",
       "        293,\n",
       "        309,\n",
       "        486,\n",
       "        764\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.397910246977935,\n",
       "      \"compression_ratio\": 1.6065573770491803,\n",
       "      \"end\": 380.64000000000004,\n",
       "      \"id\": 60,\n",
       "      \"no_speech_prob\": 2.8154952815384604e-05,\n",
       "      \"seek\": 37004,\n",
       "      \"start\": 379.52000000000004,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" as a search engine.\",\n",
       "      \"tokens\": [\n",
       "        382,\n",
       "        257,\n",
       "        3164,\n",
       "        2848,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.397910246977935,\n",
       "      \"compression_ratio\": 1.6065573770491803,\n",
       "      \"end\": 388.04,\n",
       "      \"id\": 61,\n",
       "      \"no_speech_prob\": 2.8154952815384604e-05,\n",
       "      \"seek\": 37004,\n",
       "      \"start\": 380.64000000000004,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So we forked it and implement our OWASP, a basic combination of coherent and quadrant\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        321,\n",
       "        17716,\n",
       "        292,\n",
       "        309,\n",
       "        293,\n",
       "        4445,\n",
       "        527,\n",
       "        38329,\n",
       "        3160,\n",
       "        47,\n",
       "        11,\n",
       "        257,\n",
       "        3875,\n",
       "        6562,\n",
       "        295,\n",
       "        36239,\n",
       "        293,\n",
       "        46856\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.397910246977935,\n",
       "      \"compression_ratio\": 1.6065573770491803,\n",
       "      \"end\": 390.76,\n",
       "      \"id\": 62,\n",
       "      \"no_speech_prob\": 2.8154952815384604e-05,\n",
       "      \"seek\": 37004,\n",
       "      \"start\": 388.04,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" services.\",\n",
       "      \"tokens\": [\n",
       "        3328,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.397910246977935,\n",
       "      \"compression_ratio\": 1.6065573770491803,\n",
       "      \"end\": 399.96000000000004,\n",
       "      \"id\": 63,\n",
       "      \"no_speech_prob\": 2.8154952815384604e-05,\n",
       "      \"seek\": 37004,\n",
       "      \"start\": 390.76,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So now our fork support set up all these coherent and quadrant search systems with EPI case,\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        586,\n",
       "        527,\n",
       "        17716,\n",
       "        1406,\n",
       "        992,\n",
       "        493,\n",
       "        439,\n",
       "        613,\n",
       "        36239,\n",
       "        293,\n",
       "        46856,\n",
       "        3164,\n",
       "        3652,\n",
       "        365,\n",
       "        25330,\n",
       "        40,\n",
       "        1389,\n",
       "        11\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3620251989983893,\n",
       "      \"compression_ratio\": 1.5333333333333334,\n",
       "      \"end\": 405.12,\n",
       "      \"id\": 64,\n",
       "      \"no_speech_prob\": 4.578921289066784e-05,\n",
       "      \"seek\": 39996,\n",
       "      \"start\": 399.96,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" and that's exactly how our demo worked.\",\n",
       "      \"tokens\": [\n",
       "        293,\n",
       "        300,\n",
       "        311,\n",
       "        2293,\n",
       "        577,\n",
       "        527,\n",
       "        10723,\n",
       "        2732,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3620251989983893,\n",
       "      \"compression_ratio\": 1.5333333333333334,\n",
       "      \"end\": 411.15999999999997,\n",
       "      \"id\": 65,\n",
       "      \"no_speech_prob\": 4.578921289066784e-05,\n",
       "      \"seek\": 39996,\n",
       "      \"start\": 405.12,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Internally from the technical perspective, we convert each paragraph of the text on each\",\n",
       "      \"tokens\": [\n",
       "        4844,\n",
       "        379,\n",
       "        490,\n",
       "        264,\n",
       "        6191,\n",
       "        4585,\n",
       "        11,\n",
       "        321,\n",
       "        7620,\n",
       "        1184,\n",
       "        18865,\n",
       "        295,\n",
       "        264,\n",
       "        2487,\n",
       "        322,\n",
       "        1184\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3620251989983893,\n",
       "      \"compression_ratio\": 1.5333333333333334,\n",
       "      \"end\": 416.2,\n",
       "      \"id\": 66,\n",
       "      \"no_speech_prob\": 4.578921289066784e-05,\n",
       "      \"seek\": 39996,\n",
       "      \"start\": 411.15999999999997,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" page into the vector using coherent embedding multilingual model.\",\n",
       "      \"tokens\": [\n",
       "        3028,\n",
       "        666,\n",
       "        264,\n",
       "        8062,\n",
       "        1228,\n",
       "        36239,\n",
       "        12240,\n",
       "        3584,\n",
       "        2120,\n",
       "        38219,\n",
       "        2316,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3620251989983893,\n",
       "      \"compression_ratio\": 1.5333333333333334,\n",
       "      \"end\": 422.96,\n",
       "      \"id\": 67,\n",
       "      \"no_speech_prob\": 4.578921289066784e-05,\n",
       "      \"seek\": 39996,\n",
       "      \"start\": 416.2,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Then we just index and store them in the vector database provided by quadrant.\",\n",
       "      \"tokens\": [\n",
       "        1396,\n",
       "        321,\n",
       "        445,\n",
       "        8186,\n",
       "        293,\n",
       "        3531,\n",
       "        552,\n",
       "        294,\n",
       "        264,\n",
       "        8062,\n",
       "        8149,\n",
       "        5649,\n",
       "        538,\n",
       "        46856,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3620251989983893,\n",
       "      \"compression_ratio\": 1.5333333333333334,\n",
       "      \"end\": 426.84,\n",
       "      \"id\": 68,\n",
       "      \"no_speech_prob\": 4.578921289066784e-05,\n",
       "      \"seek\": 39996,\n",
       "      \"start\": 422.96,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" And when we have a query, we do almost the same.\",\n",
       "      \"tokens\": [\n",
       "        400,\n",
       "        562,\n",
       "        321,\n",
       "        362,\n",
       "        257,\n",
       "        14581,\n",
       "        11,\n",
       "        321,\n",
       "        360,\n",
       "        1920,\n",
       "        264,\n",
       "        912,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3008803515367105,\n",
       "      \"compression_ratio\": 1.5847953216374269,\n",
       "      \"end\": 433.03999999999996,\n",
       "      \"id\": 69,\n",
       "      \"no_speech_prob\": 2.872039840440266e-05,\n",
       "      \"seek\": 42684,\n",
       "      \"start\": 426.84,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" We convert it to the embedding and after that we search to the nearest embeddings in the\",\n",
       "      \"tokens\": [\n",
       "        492,\n",
       "        7620,\n",
       "        309,\n",
       "        281,\n",
       "        264,\n",
       "        12240,\n",
       "        3584,\n",
       "        293,\n",
       "        934,\n",
       "        300,\n",
       "        321,\n",
       "        3164,\n",
       "        281,\n",
       "        264,\n",
       "        23831,\n",
       "        12240,\n",
       "        29432,\n",
       "        294,\n",
       "        264\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3008803515367105,\n",
       "      \"compression_ratio\": 1.5847953216374269,\n",
       "      \"end\": 435.2,\n",
       "      \"id\": 70,\n",
       "      \"no_speech_prob\": 2.872039840440266e-05,\n",
       "      \"seek\": 42684,\n",
       "      \"start\": 433.03999999999996,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" quadrant vector database.\",\n",
       "      \"tokens\": [\n",
       "        46856,\n",
       "        8062,\n",
       "        8149,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3008803515367105,\n",
       "      \"compression_ratio\": 1.5847953216374269,\n",
       "      \"end\": 443.71999999999997,\n",
       "      \"id\": 71,\n",
       "      \"no_speech_prob\": 2.872039840440266e-05,\n",
       "      \"seek\": 42684,\n",
       "      \"start\": 435.2,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" And magically, our link text to these vectors are quite close to the answers to the questions\",\n",
       "      \"tokens\": [\n",
       "        400,\n",
       "        39763,\n",
       "        11,\n",
       "        527,\n",
       "        2113,\n",
       "        2487,\n",
       "        281,\n",
       "        613,\n",
       "        18875,\n",
       "        366,\n",
       "        1596,\n",
       "        1998,\n",
       "        281,\n",
       "        264,\n",
       "        6338,\n",
       "        281,\n",
       "        264,\n",
       "        1651\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3008803515367105,\n",
       "      \"compression_ratio\": 1.5847953216374269,\n",
       "      \"end\": 446.52,\n",
       "      \"id\": 72,\n",
       "      \"no_speech_prob\": 2.872039840440266e-05,\n",
       "      \"seek\": 42684,\n",
       "      \"start\": 443.71999999999997,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" we put into the query.\",\n",
       "      \"tokens\": [\n",
       "        321,\n",
       "        829,\n",
       "        666,\n",
       "        264,\n",
       "        14581,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.3008803515367105,\n",
       "      \"compression_ratio\": 1.5847953216374269,\n",
       "      \"end\": 450.14,\n",
       "      \"id\": 73,\n",
       "      \"no_speech_prob\": 2.872039840440266e-05,\n",
       "      \"seek\": 42684,\n",
       "      \"start\": 446.52,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" That's a kind of the AI magic, I guess.\",\n",
       "      \"tokens\": [\n",
       "        663,\n",
       "        311,\n",
       "        257,\n",
       "        733,\n",
       "        295,\n",
       "        264,\n",
       "        7318,\n",
       "        5585,\n",
       "        11,\n",
       "        286,\n",
       "        2041,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.32167283419905035,\n",
       "      \"compression_ratio\": 1.4404761904761905,\n",
       "      \"end\": 459.12,\n",
       "      \"id\": 74,\n",
       "      \"no_speech_prob\": 4.835758591070771e-05,\n",
       "      \"seek\": 45014,\n",
       "      \"start\": 450.14,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So the future work, first of all, current implementation is super naive of search system.\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        264,\n",
       "        2027,\n",
       "        589,\n",
       "        11,\n",
       "        700,\n",
       "        295,\n",
       "        439,\n",
       "        11,\n",
       "        2190,\n",
       "        11420,\n",
       "        307,\n",
       "        1687,\n",
       "        29052,\n",
       "        295,\n",
       "        3164,\n",
       "        1185,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.32167283419905035,\n",
       "      \"compression_ratio\": 1.4404761904761905,\n",
       "      \"end\": 463.76,\n",
       "      \"id\": 75,\n",
       "      \"no_speech_prob\": 4.835758591070771e-05,\n",
       "      \"seek\": 45014,\n",
       "      \"start\": 459.12,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" Maybe we should use a bit more complex algorithm.\",\n",
       "      \"tokens\": [\n",
       "        2704,\n",
       "        321,\n",
       "        820,\n",
       "        764,\n",
       "        257,\n",
       "        857,\n",
       "        544,\n",
       "        3997,\n",
       "        9284,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.32167283419905035,\n",
       "      \"compression_ratio\": 1.4404761904761905,\n",
       "      \"end\": 473.71999999999997,\n",
       "      \"id\": 76,\n",
       "      \"no_speech_prob\": 4.835758591070771e-05,\n",
       "      \"seek\": 45014,\n",
       "      \"start\": 463.76,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" But the funny, it takes like 300 lines of code to implement this magically high quality\",\n",
       "      \"tokens\": [\n",
       "        583,\n",
       "        264,\n",
       "        4074,\n",
       "        11,\n",
       "        309,\n",
       "        2516,\n",
       "        411,\n",
       "        6641,\n",
       "        3876,\n",
       "        295,\n",
       "        3089,\n",
       "        281,\n",
       "        4445,\n",
       "        341,\n",
       "        39763,\n",
       "        1090,\n",
       "        3125\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.32167283419905035,\n",
       "      \"compression_ratio\": 1.4404761904761905,\n",
       "      \"end\": 476.08,\n",
       "      \"id\": 77,\n",
       "      \"no_speech_prob\": 4.835758591070771e-05,\n",
       "      \"seek\": 45014,\n",
       "      \"start\": 473.71999999999997,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" search system.\",\n",
       "      \"tokens\": [\n",
       "        3164,\n",
       "        1185,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.41929615668530734,\n",
       "      \"compression_ratio\": 1.375,\n",
       "      \"end\": 480.28,\n",
       "      \"id\": 78,\n",
       "      \"no_speech_prob\": 8.99459482752718e-05,\n",
       "      \"seek\": 47608,\n",
       "      \"start\": 476.08,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" And maybe we will improve it in the future.\",\n",
       "      \"tokens\": [\n",
       "        400,\n",
       "        1310,\n",
       "        321,\n",
       "        486,\n",
       "        3470,\n",
       "        309,\n",
       "        294,\n",
       "        264,\n",
       "        2027,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.41929615668530734,\n",
       "      \"compression_ratio\": 1.375,\n",
       "      \"end\": 488.24,\n",
       "      \"id\": 79,\n",
       "      \"no_speech_prob\": 8.99459482752718e-05,\n",
       "      \"seek\": 47608,\n",
       "      \"start\": 480.28,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So but next stage, we want to automate our viscous transcription with GPT magic to allow\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        457,\n",
       "        958,\n",
       "        3233,\n",
       "        11,\n",
       "        321,\n",
       "        528,\n",
       "        281,\n",
       "        31605,\n",
       "        527,\n",
       "        1452,\n",
       "        39939,\n",
       "        35288,\n",
       "        365,\n",
       "        26039,\n",
       "        51,\n",
       "        5585,\n",
       "        281,\n",
       "        2089\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.41929615668530734,\n",
       "      \"compression_ratio\": 1.375,\n",
       "      \"end\": 498.84,\n",
       "      \"id\": 80,\n",
       "      \"no_speech_prob\": 8.99459482752718e-05,\n",
       "      \"seek\": 47608,\n",
       "      \"start\": 488.24,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" users to add videos, video pages by themselves, a bit improve our usability.\",\n",
       "      \"tokens\": [\n",
       "        5022,\n",
       "        281,\n",
       "        909,\n",
       "        2145,\n",
       "        11,\n",
       "        960,\n",
       "        7183,\n",
       "        538,\n",
       "        2969,\n",
       "        11,\n",
       "        257,\n",
       "        857,\n",
       "        3470,\n",
       "        527,\n",
       "        46878,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.43704698143935783,\n",
       "      \"compression_ratio\": 1.4393939393939394,\n",
       "      \"end\": 508.15999999999997,\n",
       "      \"id\": 81,\n",
       "      \"no_speech_prob\": 2.1578744053840637e-05,\n",
       "      \"seek\": 49884,\n",
       "      \"start\": 498.84,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" And more than we will try to make your answer your questions using data stored in our knowledge\",\n",
       "      \"tokens\": [\n",
       "        400,\n",
       "        544,\n",
       "        813,\n",
       "        321,\n",
       "        486,\n",
       "        853,\n",
       "        281,\n",
       "        652,\n",
       "        428,\n",
       "        1867,\n",
       "        428,\n",
       "        1651,\n",
       "        1228,\n",
       "        1412,\n",
       "        12187,\n",
       "        294,\n",
       "        527,\n",
       "        3601\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.43704698143935783,\n",
       "      \"compression_ratio\": 1.4393939393939394,\n",
       "      \"end\": 509.67999999999995,\n",
       "      \"id\": 82,\n",
       "      \"no_speech_prob\": 2.1578744053840637e-05,\n",
       "      \"seek\": 49884,\n",
       "      \"start\": 508.15999999999997,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" management.\",\n",
       "      \"tokens\": [\n",
       "        4592,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.43704698143935783,\n",
       "      \"compression_ratio\": 1.4393939393939394,\n",
       "      \"end\": 511.28,\n",
       "      \"id\": 83,\n",
       "      \"no_speech_prob\": 2.1578744053840637e-05,\n",
       "      \"seek\": 49884,\n",
       "      \"start\": 509.67999999999995,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" That's all for today.\",\n",
       "      \"tokens\": [\n",
       "        663,\n",
       "        311,\n",
       "        439,\n",
       "        337,\n",
       "        965,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.43704698143935783,\n",
       "      \"compression_ratio\": 1.4393939393939394,\n",
       "      \"end\": 514.76,\n",
       "      \"id\": 84,\n",
       "      \"no_speech_prob\": 2.1578744053840637e-05,\n",
       "      \"seek\": 49884,\n",
       "      \"start\": 511.28,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" By the way, we are going to participate in our Zagreco-Tron.\",\n",
       "      \"tokens\": [\n",
       "        3146,\n",
       "        264,\n",
       "        636,\n",
       "        11,\n",
       "        321,\n",
       "        366,\n",
       "        516,\n",
       "        281,\n",
       "        8197,\n",
       "        294,\n",
       "        527,\n",
       "        1176,\n",
       "        559,\n",
       "        265,\n",
       "        1291,\n",
       "        12,\n",
       "        51,\n",
       "        2044,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.43704698143935783,\n",
       "      \"compression_ratio\": 1.4393939393939394,\n",
       "      \"end\": 517.8399999999999,\n",
       "      \"id\": 85,\n",
       "      \"no_speech_prob\": 2.1578744053840637e-05,\n",
       "      \"seek\": 49884,\n",
       "      \"start\": 514.76,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" So if you're interested, please join our team.\",\n",
       "      \"tokens\": [\n",
       "        407,\n",
       "        498,\n",
       "        291,\n",
       "        434,\n",
       "        3102,\n",
       "        11,\n",
       "        1767,\n",
       "        3917,\n",
       "        527,\n",
       "        1469,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -0.43704698143935783,\n",
       "      \"compression_ratio\": 1.4393939393939394,\n",
       "      \"end\": 521.76,\n",
       "      \"id\": 86,\n",
       "      \"no_speech_prob\": 2.1578744053840637e-05,\n",
       "      \"seek\": 49884,\n",
       "      \"start\": 517.8399999999999,\n",
       "      \"temperature\": 0.0,\n",
       "      \"text\": \" We have a huge project moving forward.\",\n",
       "      \"tokens\": [\n",
       "        492,\n",
       "        362,\n",
       "        257,\n",
       "        2603,\n",
       "        1716,\n",
       "        2684,\n",
       "        2128,\n",
       "        13\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    },\n",
       "    {\n",
       "      \"avg_logprob\": -1.2016736666361492,\n",
       "      \"compression_ratio\": 0.5,\n",
       "      \"end\": 529.18,\n",
       "      \"id\": 87,\n",
       "      \"no_speech_prob\": 3.488983929855749e-05,\n",
       "      \"seek\": 52176,\n",
       "      \"start\": 521.76,\n",
       "      \"temperature\": 1.0,\n",
       "      \"text\": \" See you.\",\n",
       "      \"tokens\": [\n",
       "        50364,\n",
       "        3008,\n",
       "        291,\n",
       "        13,\n",
       "        50735\n",
       "      ],\n",
       "      \"transient\": false\n",
       "    }\n",
       "  ],\n",
       "  \"task\": \"transcribe\",\n",
       "  \"text\": \"Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia. So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some structure here, and whatever, just editor for the small pages. So we converted demos of a few teams into this Wikipedia format using Whisper and some magic from GPT to make them much more readable. But what is more important, we allow them to be searchable by semantic search. So we use a multilingual semantic search, and I will show you how it works. We have some project, Ask Quran, let's try to formulate it in Russian, like a questions about God. Yeah, we have Ask Quran project. It's more delay on loading. I will skip a bit. Seems like it's quite a good match about questions about God in Russian to the English text. So let me show you how our system, our Wikipedias works with video. So when video is playing, we highlight the part of the video, or you can jump to the part of the video from the video. We could jump to the timestamp in the video, clicking to the text, or you can switch the video part and see which part of the text it is. Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search. So I will try to write in Spanish, not as personales, which is personal nouns, and see what we get. So we have a daily journaling app presentation. To speed up, I will click here. In conversational daily journaling experience, where we hope to enable users to write your way to a better day. That seems quite a good match for the questions, not as personales. Well, enough for the glamour, back to slides. Before jumping into the technical details, I would like to say that this knowledge management market is huge. It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two digits figures, depending on which report you read. And it's quite a good trend to disrupt this market by new AI solutions or AI technologies, like our IHANA. So that could not be just a fancy toy with technology, but it could be a part of the business because this solution is applicable in every enterprise environment. Well, then it could help the people who cannot listen to or watch videos to read information. And I would say you can help busy managers to spend some time reading instead of watching videos. Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the webcasts, but they are not searchable, so I guess it's not accessible. Well, how it was done. As a base, we use an open source knowledge management system, Wikijs. It's quite commercially used and popular, written in JavaScript. It already supports some amount of search engines that you can set up and it will use as a search engine. So we forked it and implement our OWASP, a basic combination of coherent and quadrant services. So now our fork support set up all these coherent and quadrant search systems with EPI case, and that's exactly how our demo worked. Internally from the technical perspective, we convert each paragraph of the text on each page into the vector using coherent embedding multilingual model. Then we just index and store them in the vector database provided by quadrant. And when we have a query, we do almost the same. We convert it to the embedding and after that we search to the nearest embeddings in the quadrant vector database. And magically, our link text to these vectors are quite close to the answers to the questions we put into the query. That's a kind of the AI magic, I guess. So the future work, first of all, current implementation is super naive of search system. Maybe we should use a bit more complex algorithm. But the funny, it takes like 300 lines of code to implement this magically high quality search system. And maybe we will improve it in the future. So but next stage, we want to automate our viscous transcription with GPT magic to allow users to add videos, video pages by themselves, a bit improve our usability. And more than we will try to make your answer your questions using data stored in our knowledge management. That's all for today. By the way, we are going to participate in our Zagreco-Tron. So if you're interested, please join our team. We have a huge project moving forward. See you.\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('transcript.json', 'w') as f:\n",
    "    json.dump(transcript, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{~0.00} Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise {~8.84} \n",
      " Assistants. {~9.84} \n",
      " Juan is responsible for knowing everything in your company, but first things first, our {~16.56} \n",
      " team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather {~23.40} \n",
      " together to solve one simple problem. {~26.00} \n",
      " The problem is that we are really spoiled by quality of Google search. {~30.92} \n",
      " So spoiled that every time we need to find something in our corporate systems, we suffer, {~37.68} \n",
      " because they usually use the keyword search, the old style, not the modern one that Google, {~43.28} \n",
      " not modern one, semantic, that Google use. {~47.86} \n",
      " But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, {~55.44} \n",
      " like you are looking now. {~58.04} \n",
      " Imagine that all video presentation of business processes, systems, solutions, or ever greater {~65.32} \n",
      " ideas in your company will be converted into the Wikipedia pages and will be searchable {~72.48} \n",
      " by modern search engines. {~75.72} \n",
      " And that's exactly our, what Juan Holloway is doing. {~80.48} \n",
      " Let me show you. {~85.76} \n",
      " To make a demo, we convert some results from previous hackathons into our Wikipedia. {~94.46} \n",
      " So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some {~102.36} \n",
      " structure here, and whatever, just editor for the small pages. {~108.60} \n",
      " So we converted demos of a few teams into this Wikipedia format using Whisper and some {~117.68} \n",
      " magic from GPT to make them much more readable. {~121.68} \n",
      " But what is more important, we allow them to be searchable by semantic search. {~129.08} \n",
      " So we use a multilingual semantic search, and I will show you how it works. {~134.32} \n",
      " We have some project, Ask Quran, let's try to formulate it in Russian, like a questions {~141.20} \n",
      " about God. {~144.92} \n",
      " Yeah, we have Ask Quran project. {~155.92} \n",
      " It's more delay on loading. {~164.92} \n",
      " I will skip a bit. {~178.68} \n",
      " Seems like it's quite a good match about questions about God in Russian to the English text. {~185.36} \n",
      " So let me show you how our system, our Wikipedias works with video. {~193.70} \n",
      " So when video is playing, we highlight the part of the video, or you can jump to the {~205.64} \n",
      " part of the video from the video. {~212.72} \n",
      " We could jump to the timestamp in the video, clicking to the text, or you can switch the {~219.68} \n",
      " video part and see which part of the text it is. {~223.28} \n",
      " Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search. {~231.36} \n",
      " So I will try to write in Spanish, not as personales, which is personal nouns, and see {~238.88} \n",
      " what we get. {~239.88} \n",
      " So we have a daily journaling app presentation. {~247.80} \n",
      " To speed up, I will click here. {~251.08} \n",
      " In conversational daily journaling experience, where we hope to enable users to write your {~258.32} \n",
      " way to a better day. {~263.36} \n",
      " That seems quite a good match for the questions, not as personales. {~267.88} \n",
      " Well, enough for the glamour, back to slides. {~273.72} \n",
      " Before jumping into the technical details, I would like to say that this knowledge management {~280.00} \n",
      " market is huge. {~282.28} \n",
      " It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two {~291.44} \n",
      " digits figures, depending on which report you read. {~296.12} \n",
      " And it's quite a good trend to disrupt this market by new AI solutions or AI technologies, {~306.32} \n",
      " like our IHANA. {~308.52} \n",
      " So that could not be just a fancy toy with technology, but it could be a part of the {~315.68} \n",
      " business because this solution is applicable in every enterprise environment. {~322.00} \n",
      " Well, then it could help the people who cannot listen to or watch videos to read information. {~331.72} \n",
      " And I would say you can help busy managers to spend some time reading instead of watching {~340.40} \n",
      " videos. {~341.80} \n",
      " Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the {~349.32} \n",
      " webcasts, but they are not searchable, so I guess it's not accessible. {~355.56} \n",
      " Well, how it was done. {~360.04} \n",
      " As a base, we use an open source knowledge management system, Wikijs. {~363.44} \n",
      " It's quite commercially used and popular, written in JavaScript. {~370.04} \n",
      " It already supports some amount of search engines that you can set up and it will use {~379.52} \n",
      " as a search engine. {~380.64} \n",
      " So we forked it and implement our OWASP, a basic combination of coherent and quadrant {~388.04} \n",
      " services. {~390.76} \n",
      " So now our fork support set up all these coherent and quadrant search systems with EPI case, {~399.96} \n",
      " and that's exactly how our demo worked. {~405.12} \n",
      " Internally from the technical perspective, we convert each paragraph of the text on each {~411.16} \n",
      " page into the vector using coherent embedding multilingual model. {~416.20} \n",
      " Then we just index and store them in the vector database provided by quadrant. {~422.96} \n",
      " And when we have a query, we do almost the same. {~426.84} \n",
      " We convert it to the embedding and after that we search to the nearest embeddings in the {~433.04} \n",
      " quadrant vector database. {~435.20} \n",
      " And magically, our link text to these vectors are quite close to the answers to the questions {~443.72} \n",
      " we put into the query. {~446.52} \n",
      " That's a kind of the AI magic, I guess. {~450.14} \n",
      " So the future work, first of all, current implementation is super naive of search system. {~459.12} \n",
      " Maybe we should use a bit more complex algorithm. {~463.76} \n",
      " But the funny, it takes like 300 lines of code to implement this magically high quality {~473.72} \n",
      " search system. {~476.08} \n",
      " And maybe we will improve it in the future. {~480.28} \n",
      " So but next stage, we want to automate our viscous transcription with GPT magic to allow {~488.24} \n",
      " users to add videos, video pages by themselves, a bit improve our usability. {~498.84} \n",
      " And more than we will try to make your answer your questions using data stored in our knowledge {~508.16} \n",
      " management. {~509.68} \n",
      " That's all for today. {~511.28} \n",
      " By the way, we are going to participate in our Zagreco-Tron. {~514.76} \n",
      " So if you're interested, please join our team. {~517.84} \n",
      " We have a huge project moving forward. {~521.76} \n",
      " See you. {~529.18} \n"
     ]
    }
   ],
   "source": [
    "last = -100.0\n",
    "lines = []\n",
    "\n",
    "for seg in transcript[\"segments\"]:\n",
    "    \n",
    "    line = f\"{seg['text']} {{~{seg['end']:.2f}}} \"\n",
    "    long_pause = seg[\"start\"] - last > 1.0\n",
    "    if long_pause:\n",
    "        line = f\"{{~{seg['start']:.2f}}}\" + line\n",
    "    last = seg[\"end\"]\n",
    "    lines.append(line)\n",
    "    \n",
    "\n",
    "print(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia. So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some structure here, and whatever, just editor for the small pages. So we converted demos of a few teams into this Wikipedia format using Whisper and some magic from GPT to make them much more readable. But what is more important, we allow them to be searchable by semantic search. So we use a multilingual semantic search, and I will show you how it works. We have some project, Ask Quran, let's try to formulate it in Russian, like a questions about God. Yeah, we have Ask Quran project. It's more delay on loading. I will skip a bit. Seems like it's quite a good match about questions about God in Russian to the English text. So let me show you how our system, our Wikipedias works with video. So when video is playing, we highlight the part of the video, or you can jump to the part of the video from the video. We could jump to the timestamp in the video, clicking to the text, or you can switch the video part and see which part of the text it is. Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search. So I will try to write in Spanish, not as personales, which is personal nouns, and see what we get. So we have a daily journaling app presentation. To speed up, I will click here. In conversational daily journaling experience, where we hope to enable users to write your way to a better day. That seems quite a good match for the questions, not as personales. Well, enough for the glamour, back to slides. Before jumping into the technical details, I would like to say that this knowledge management market is huge. It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two digits figures, depending on which report you read. And it's quite a good trend to disrupt this market by new AI solutions or AI technologies, like our IHANA. So that could not be just a fancy toy with technology, but it could be a part of the business because this solution is applicable in every enterprise environment. Well, then it could help the people who cannot listen to or watch videos to read information. And I would say you can help busy managers to spend some time reading instead of watching videos. Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the webcasts, but they are not searchable, so I guess it's not accessible. Well, how it was done. As a base, we use an open source knowledge management system, Wikijs. It's quite commercially used and popular, written in JavaScript. It already supports some amount of search engines that you can set up and it will use as a search engine. So we forked it and implement our OWASP, a basic combination of coherent and quadrant services. So now our fork support set up all these coherent and quadrant search systems with EPI case, and that's exactly how our demo worked. Internally from the technical perspective, we convert each paragraph of the text on each page into the vector using coherent embedding multilingual model. Then we just index and store them in the vector database provided by quadrant. And when we have a query, we do almost the same. We convert it to the embedding and after that we search to the nearest embeddings in the quadrant vector database. And magically, our link text to these vectors are quite close to the answers to the questions we put into the query. That's a kind of the AI magic, I guess. So the future work, first of all, current implementation is super naive of search system. Maybe we should use a bit more complex algorithm. But the funny, it takes like 300 lines of code to implement this magically high quality search system. And maybe we will improve it in the future. So but next stage, we want to automate our viscous transcription with GPT magic to allow users to add videos, video pages by themselves, a bit improve our usability. And more than we will try to make your answer your questions using data stored in our knowledge management. That's all for today. By the way, we are going to participate in our Zagreco-Tron. So if you're interested, please join our team. We have a huge project moving forward. See you.\n"
     ]
    }
   ],
   "source": [
    "text = \"\".join([ seg[\"text\"]\n",
    "for seg in transcript[\"segments\"]])\n",
    "\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assistants.\n",
      " together to solve one simple problem.\n",
      " The problem is that we are really spoiled by quality of Google search.\n",
      " not modern one, semantic, that Google use.\n",
      " like you are looking now.\n",
      " by modern search engines.\n",
      " And that's exactly our, what Juan Holloway is doing.\n",
      " Let me show you.\n",
      " To make a demo, we convert some results from previous hackathons into our Wikipedia.\n",
      " structure here, and whatever, just editor for the small pages.\n",
      " magic from GPT to make them much more readable.\n",
      " But what is more important, we allow them to be searchable by semantic search.\n",
      " So we use a multilingual semantic search, and I will show you how it works.\n",
      " about God.\n",
      " Yeah, we have Ask Quran project.\n",
      " It's more delay on loading.\n",
      " I will skip a bit.\n",
      " Seems like it's quite a good match about questions about God in Russian to the English text.\n",
      " So let me show you how our system, our Wikipedias works with video.\n",
      " part of the video from the video.\n",
      " video part and see which part of the text it is.\n",
      " Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search.\n",
      " what we get.\n",
      " So we have a daily journaling app presentation.\n",
      " To speed up, I will click here.\n",
      " way to a better day.\n",
      " That seems quite a good match for the questions, not as personales.\n",
      " Well, enough for the glamour, back to slides.\n",
      " market is huge.\n",
      " digits figures, depending on which report you read.\n",
      " like our IHANA.\n",
      " business because this solution is applicable in every enterprise environment.\n",
      " Well, then it could help the people who cannot listen to or watch videos to read information.\n",
      " videos.\n",
      " webcasts, but they are not searchable, so I guess it's not accessible.\n",
      " Well, how it was done.\n",
      " As a base, we use an open source knowledge management system, Wikijs.\n",
      " It's quite commercially used and popular, written in JavaScript.\n",
      " as a search engine.\n",
      " services.\n",
      " and that's exactly how our demo worked.\n",
      " page into the vector using coherent embedding multilingual model.\n",
      " Then we just index and store them in the vector database provided by quadrant.\n",
      " And when we have a query, we do almost the same.\n",
      " quadrant vector database.\n",
      " we put into the query.\n",
      " That's a kind of the AI magic, I guess.\n",
      " So the future work, first of all, current implementation is super naive of search system.\n",
      " Maybe we should use a bit more complex algorithm.\n",
      " search system.\n",
      " And maybe we will improve it in the future.\n",
      " users to add videos, video pages by themselves, a bit improve our usability.\n",
      " management.\n",
      " That's all for today.\n",
      " By the way, we are going to participate in our Zagreco-Tron.\n",
      " So if you're interested, please join our team.\n",
      " We have a huge project moving forward.\n",
      " See you.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([ seg[\"text\"]\n",
    "for seg in transcript[\"segments\"] if seg[\"text\"].endswith(\".\") ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1097\n",
      " Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia.\n",
      "1 1097 902\n",
      " So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some structure here, and whatever, just editor for the small pages. So we converted demos of a few teams into this Wikipedia format using Whisper and some magic from GPT to make them much more readable. But what is more important, we allow them to be searchable by semantic search. So we use a multilingual semantic search, and I will show you how it works. We have some project, Ask Quran, let's try to formulate it in Russian, like a questions about God. Yeah, we have Ask Quran project. It's more delay on loading. I will skip a bit. Seems like it's quite a good match about questions about God in Russian to the English text. So let me show you how our system, our Wikipedias works with video. So when video is playing, we highlight the part of the video, or you can jump to the part of the video from the video.\n",
      "2 1999 999\n",
      " We could jump to the timestamp in the video, clicking to the text, or you can switch the video part and see which part of the text it is. Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search. So I will try to write in Spanish, not as personales, which is personal nouns, and see what we get. So we have a daily journaling app presentation. To speed up, I will click here. In conversational daily journaling experience, where we hope to enable users to write your way to a better day. That seems quite a good match for the questions, not as personales. Well, enough for the glamour, back to slides. Before jumping into the technical details, I would like to say that this knowledge management market is huge. It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two digits figures, depending on which report you read. And it's quite a good trend to disrupt this market by new AI solutions or AI technologies, like our IHANA.\n",
      "3 2998 1008\n",
      " So that could not be just a fancy toy with technology, but it could be a part of the business because this solution is applicable in every enterprise environment. Well, then it could help the people who cannot listen to or watch videos to read information. And I would say you can help busy managers to spend some time reading instead of watching videos. Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the webcasts, but they are not searchable, so I guess it's not accessible. Well, how it was done. As a base, we use an open source knowledge management system, Wikijs. It's quite commercially used and popular, written in JavaScript. It already supports some amount of search engines that you can set up and it will use as a search engine. So we forked it and implement our OWASP, a basic combination of coherent and quadrant services. So now our fork support set up all these coherent and quadrant search systems with EPI case, and that's exactly how our demo worked.\n",
      "4 4006 1008\n",
      " Internally from the technical perspective, we convert each paragraph of the text on each page into the vector using coherent embedding multilingual model. Then we just index and store them in the vector database provided by quadrant. And when we have a query, we do almost the same. We convert it to the embedding and after that we search to the nearest embeddings in the quadrant vector database. And magically, our link text to these vectors are quite close to the answers to the questions we put into the query. That's a kind of the AI magic, I guess. So the future work, first of all, current implementation is super naive of search system. Maybe we should use a bit more complex algorithm. But the funny, it takes like 300 lines of code to implement this magically high quality search system. And maybe we will improve it in the future. So but next stage, we want to automate our viscous transcription with GPT magic to allow users to add videos, video pages by themselves, a bit improve our usability.\n",
      "5 5014 286\n",
      " And more than we will try to make your answer your questions using data stored in our knowledge management. That's all for today. By the way, we are going to participate in our Zagreco-Tron. So if you're interested, please join our team. We have a huge project moving forward. See you.\n",
      " Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia. So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some structure here, and whatever, just editor for the small pages. So we converted demos of a few teams into this Wikipedia format using Whisper and some magic from GPT to make them much more readable. But what is more important, we allow them to be searchable by semantic search. So we use a multilingual semantic search, and I will show you how it works. We have some project, Ask Quran, let's try to formulate it in Russian, like a questions about God. Yeah, we have Ask Quran project. It's more delay on loading. I will skip a bit. Seems like it's quite a good match about questions about God in Russian to the English text. So let me show you how our system, our Wikipedias works with video. So when video is playing, we highlight the part of the video, or you can jump to the part of the video from the video. We could jump to the timestamp in the video, clicking to the text, or you can switch the video part and see which part of the text it is. Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search. So I will try to write in Spanish, not as personales, which is personal nouns, and see what we get. So we have a daily journaling app presentation. To speed up, I will click here. In conversational daily journaling experience, where we hope to enable users to write your way to a better day. That seems quite a good match for the questions, not as personales. Well, enough for the glamour, back to slides. Before jumping into the technical details, I would like to say that this knowledge management market is huge. It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two digits figures, depending on which report you read. And it's quite a good trend to disrupt this market by new AI solutions or AI technologies, like our IHANA. So that could not be just a fancy toy with technology, but it could be a part of the business because this solution is applicable in every enterprise environment. Well, then it could help the people who cannot listen to or watch videos to read information. And I would say you can help busy managers to spend some time reading instead of watching videos. Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the webcasts, but they are not searchable, so I guess it's not accessible. Well, how it was done. As a base, we use an open source knowledge management system, Wikijs. It's quite commercially used and popular, written in JavaScript. It already supports some amount of search engines that you can set up and it will use as a search engine. So we forked it and implement our OWASP, a basic combination of coherent and quadrant services. So now our fork support set up all these coherent and quadrant search systems with EPI case, and that's exactly how our demo worked. Internally from the technical perspective, we convert each paragraph of the text on each page into the vector using coherent embedding multilingual model. Then we just index and store them in the vector database provided by quadrant. And when we have a query, we do almost the same. We convert it to the embedding and after that we search to the nearest embeddings in the quadrant vector database. And magically, our link text to these vectors are quite close to the answers to the questions we put into the query. That's a kind of the AI magic, I guess. So the future work, first of all, current implementation is super naive of search system. Maybe we should use a bit more complex algorithm. But the funny, it takes like 300 lines of code to implement this magically high quality search system. And maybe we will improve it in the future. So but next stage, we want to automate our viscous transcription with GPT magic to allow users to add videos, video pages by themselves, a bit improve our usability. And more than we will try to make your answer your questions using data stored in our knowledge management. That's all for today. By the way, we are going to participate in our Zagreco-Tron. So if you're interested, please join our team. We have a huge project moving forward. See you.\n",
      " Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia. So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some structure here, and whatever, just editor for the small pages. So we converted demos of a few teams into this Wikipedia format using Whisper and some magic from GPT to make them much more readable. But what is more important, we allow them to be searchable by semantic search. So we use a multilingual semantic search, and I will show you how it works. We have some project, Ask Quran, let's try to formulate it in Russian, like a questions about God. Yeah, we have Ask Quran project. It's more delay on loading. I will skip a bit. Seems like it's quite a good match about questions about God in Russian to the English text. So let me show you how our system, our Wikipedias works with video. So when video is playing, we highlight the part of the video, or you can jump to the part of the video from the video. We could jump to the timestamp in the video, clicking to the text, or you can switch the video part and see which part of the text it is. Oh, it's a bit out of scope of current hackathon, and we are talking about semantic search. So I will try to write in Spanish, not as personales, which is personal nouns, and see what we get. So we have a daily journaling app presentation. To speed up, I will click here. In conversational daily journaling experience, where we hope to enable users to write your way to a better day. That seems quite a good match for the questions, not as personales. Well, enough for the glamour, back to slides. Before jumping into the technical details, I would like to say that this knowledge management market is huge. It's measured in hundreds of billions of dollars a year, and it is growing quite fast for two digits figures, depending on which report you read. And it's quite a good trend to disrupt this market by new AI solutions or AI technologies, like our IHANA. So that could not be just a fancy toy with technology, but it could be a part of the business because this solution is applicable in every enterprise environment. Well, then it could help the people who cannot listen to or watch videos to read information. And I would say you can help busy managers to spend some time reading instead of watching videos. Like here, we can apply in Blablab AI because we have a lot of knowledge recorded to the webcasts, but they are not searchable, so I guess it's not accessible. Well, how it was done. As a base, we use an open source knowledge management system, Wikijs. It's quite commercially used and popular, written in JavaScript. It already supports some amount of search engines that you can set up and it will use as a search engine. So we forked it and implement our OWASP, a basic combination of coherent and quadrant services. So now our fork support set up all these coherent and quadrant search systems with EPI case, and that's exactly how our demo worked. Internally from the technical perspective, we convert each paragraph of the text on each page into the vector using coherent embedding multilingual model. Then we just index and store them in the vector database provided by quadrant. And when we have a query, we do almost the same. We convert it to the embedding and after that we search to the nearest embeddings in the quadrant vector database. And magically, our link text to these vectors are quite close to the answers to the questions we put into the query. That's a kind of the AI magic, I guess. So the future work, first of all, current implementation is super naive of search system. Maybe we should use a bit more complex algorithm. But the funny, it takes like 300 lines of code to implement this magically high quality search system. And maybe we will improve it in the future. So but next stage, we want to automate our viscous transcription with GPT magic to allow users to add videos, video pages by themselves, a bit improve our usability. And more than we will try to make your answer your questions using data stored in our knowledge management. That's all for today. By the way, we are going to participate in our Zagreco-Tron. So if you're interested, please join our team. We have a huge project moving forward. See you.\n"
     ]
    }
   ],
   "source": [
    "def next_block(transcript, start_index = 0, limit = 200):\n",
    "    \n",
    "    text = []\n",
    "    words = 0\n",
    "    seg_index = 0\n",
    "    for i in range (start_index, len( transcript[\"segments\"])):\n",
    "        line = transcript[\"segments\"][i][\"text\"]\n",
    "        text.append(line)\n",
    "        words += len(line.split(\" \"))\n",
    "        if line[-1] in [\".\",\"?\",\"!\"]:\n",
    "            seg_index = len (text)\n",
    "        if words > limit:\n",
    "            break\n",
    "    else:\n",
    "        seg_index = len (text)\n",
    "        assert i ==len( transcript[\"segments\"]) -1\n",
    "        \n",
    "    text = \"\".join(text[:seg_index])\n",
    "    return text, start_index + seg_index\n",
    "\n",
    "\n",
    "\n",
    "full_text =\"\".join([ seg[\"text\"] for seg in transcript[\"segments\"]])\n",
    "blocks = []\n",
    "seg_index = 0\n",
    "while seg_index < len(transcript[\"segments\"]):\n",
    "    block, seg_index = next_block(transcript,seg_index) \n",
    "    blocks.append(block)\n",
    "text = \"\".join(blocks)\n",
    "j= 0\n",
    "for i, block in enumerate(blocks):\n",
    "    bloc_len = len(block)\n",
    "    print (i, j, bloc_len)\n",
    "    print (block)\n",
    "    assert full_text[j:j+bloc_len] == block, f\"{full_text[j:j+bloc_len]} \\n {block}\"\n",
    "    j += bloc_len\n",
    "    \n",
    "print (text)\n",
    "print (full_text)\n",
    "assert full_text == text , f\"{full_text} \\n {text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a transcription assistant.\n",
    "Your goal is to make provided text readable while preserving original content as much as possible.\n",
    "User will give you an exact task, try to execute as accurately as you can.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt (text, context=None):\n",
    "    if context is None:\n",
    "        prompt = f\"\"\"\n",
    "            You are a transcription assistant. Process the given text into Markdown format fully, without any changes to the text. \n",
    "            Put two carriage returns between paragraphs that make sense and are no longer than 100 tokens.\n",
    "            Group paragraphs into sections by inserting meaningful headings in Markdown format (line starting with ##). \n",
    "            --------------text----------------\n",
    "            {text}\n",
    "            \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "            You are a transcription assistant.\n",
    "            You'll be given a previous formatted fragment as context and the next portion of text to process as new text. \n",
    "            Process a new text into Markdown format fully, without any changes to the text. \n",
    "            Put two carriage returns between paragraphs that make sense and are no longer than 100 tokens.\n",
    "            Group paragraphs into sections by inserting meaningful headings in Markdown format (line starting with ##). \n",
    "\n",
    "            ------------context---------------\n",
    "            {context}\n",
    "            -----------new text---------------\n",
    "            {text}\n",
    "            \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from openai import util\n",
    "from openai.api_resources.abstract.engine_api_resource import EngineAPIResource\n",
    "from openai.error import TryAgain\n",
    "\n",
    "\n",
    "class ChatCompletion(EngineAPIResource):\n",
    "    engine_required = False\n",
    "    OBJECT_NAME = \"chat.completions\"\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new chat completion for the provided messages and parameters.\n",
    "\n",
    "        See https://platform.openai.com/docs/api-reference/chat-completions/create\n",
    "        for a list of valid parameters.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        timeout = kwargs.pop(\"timeout\", None)\n",
    "        if timeout is not None:\n",
    "            kwargs[\"timeout\"]= timeout\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                return super().create(*args, **kwargs)\n",
    "            except TryAgain as e:\n",
    "                if timeout is not None and time.time() > start + timeout:\n",
    "                    raise\n",
    "\n",
    "                util.log_info(\"Waiting for model to warm up\", error=e)\n",
    "\n",
    "    @classmethod\n",
    "    async def acreate(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new chat completion for the provided messages and parameters.\n",
    "\n",
    "        See https://platform.openai.com/docs/api-reference/chat-completions/create\n",
    "        for a list of valid parameters.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        timeout = kwargs.pop(\"timeout\", None)\n",
    "        if timeout is not None:\n",
    "            kwargs[\"timeout\"]= timeout\n",
    "            \n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                return await super().acreate(*args, **kwargs)\n",
    "            except TryAgain as e:\n",
    "                if timeout is not None and time.time() > start + timeout:\n",
    "                    raise\n",
    "\n",
    "                util.log_info(\"Waiting for model to warm up\", error=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(system_prompt,user_prompt,  model=\"gpt-3.5-turbo\", temperature=0.0,attempts=5):\n",
    "    \n",
    "    for _ in range(attempts):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                timeout = 60,\n",
    "            )\n",
    "            return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except openai.error.Timeout as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    raise Exception(\"Failed to get response\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_result = process(\n",
    "        system_prompt = system_prompt,\n",
    "        user_prompt= get_prompt(text),\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Demo\n",
      "Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia.\n"
     ]
    }
   ],
   "source": [
    "def get_last_section(formatted_result):\n",
    "    sections = formatted_result.split(\"##\")\n",
    "    return \"##\"+sections[-1]\n",
    "\n",
    "print(get_last_section(formatted_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Introduction\n",
      "Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. \n",
      "\n",
      "## Our Team\n",
      "Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem.\n",
      "\n",
      "## The Problem\n",
      "The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use.\n",
      "\n",
      "But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. \n",
      "\n",
      "## The Solution\n",
      "Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. \n",
      "\n",
      "## Demo\n",
      "Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia.\n"
     ]
    }
   ],
   "source": [
    "print(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m         text_with_timestamps\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(line))\n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(text_with_timestamps), seg_index\n\u001b[0;32m---> 32\u001b[0m text_with_timestamps, seg_index  \u001b[39m=\u001b[39m add_timestamps(formatted_result, transcript)  \n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m (text_with_timestamps)\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m (transcript[\u001b[39m\"\u001b[39m\u001b[39msegments\u001b[39m\u001b[39m\"\u001b[39m][seg_index][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[62], line 17\u001b[0m, in \u001b[0;36madd_timestamps\u001b[0;34m(formatted_result, transcript, seg_index)\u001b[0m\n\u001b[1;32m     15\u001b[0m seg_text \u001b[39m=\u001b[39m seg[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlstrip()\n\u001b[1;32m     16\u001b[0m \u001b[39m#print (seg_text)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mwhile\u001b[39;00m par[pos] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos\u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(par):\n\u001b[1;32m     18\u001b[0m     pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39m#print (par[pos:])\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "def add_timestamps(formatted_result, transcript,seg_index = 0):\n",
    "\n",
    "    text_with_timestamps = []\n",
    "    add_start_timestamp = True\n",
    "    for par in formatted_result.split(\"\\n\"):\n",
    "        if par.startswith(\"##\") or par ==\"\":\n",
    "            text_with_timestamps.append(par)\n",
    "            add_start_timestamp = True\n",
    "            continue\n",
    "        line = []\n",
    "        # find segments in the paragraph\n",
    "        pos = 0\n",
    "        while pos< len(par):\n",
    "            seg = transcript[\"segments\"][seg_index]\n",
    "            seg_text = seg[\"text\"].lstrip()\n",
    "            #print (seg_text)\n",
    "            while par[pos] == \" \" and pos< len(par):\n",
    "                pos += 1\n",
    "            #print (par[pos:])\n",
    "            assert par[pos:].startswith(seg_text), f\"{seg_text}\\n{par[pos:]} \"\n",
    "            seg_index += 1\n",
    "            pos += len(seg_text)\n",
    "            span  = f\"{seg_text} {{~{seg['end']:.2f}}} \"\n",
    "            if add_start_timestamp:\n",
    "                span = f\"{{~{seg['start']:.2f}}} \" + span\n",
    "            add_start_timestamp = False\n",
    "            line.append(span)\n",
    "            \n",
    "        text_with_timestamps.append(\"\".join(line))\n",
    "    return \"\\n\".join(text_with_timestamps), seg_index\n",
    "\n",
    "text_with_timestamps, seg_index  = add_timestamps(formatted_result, transcript)  \n",
    "print (text_with_timestamps)\n",
    "print (transcript[\"segments\"][seg_index][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise\n",
      "16  To make a demo, we convert some results from previous hackathons into our Wikipedia.\n",
      "\n",
      "            You are a transcription assistant. Process the given text into Markdown format fully, without any changes to the text. \n",
      "            Put two carriage returns between paragraphs that make sense and are no longer than 100 tokens.\n",
      "            Group paragraphs into sections by inserting meaningful headings in Markdown format (line starting with ##). \n",
      "            --------------text----------------\n",
      "             Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia.\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"p\\xd7\\x0c\\x87\\x05\\xc8J\\x01\\xac{\\x16\\xdcc\\xf9e\\xb3\\x04\\xdf\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t\", b'7.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00']\n",
      "Bad pipe message: %s [b'\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06\\x02']\n",
      "Bad pipe message: %s [b\"Fk\\x90?\\x8e\\x12\\x03qV\\x1d*\\x97\\xa3\\xd8\\xe5\\x19D\\xa3\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\"]\n",
      "Bad pipe message: %s [b'\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c']\n",
      "Bad pipe message: %s [b'\\xabM\\xa9$~\\x9f\\xcd\\xe8', b'`\\xa7\\xe6e\\x9a\\x87', b'\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00', b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b\"\\xbc8\\xd8\\x8d\\x8d-yG\\xb1\\xa8\\x12\\xcf\\x86`\\xcd\\xe0\\xa9\\x03\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\"]\n",
      "Bad pipe message: %s [b'\\x04\\x01\\x04\\x02', b'\\x03\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\n",
      "Bad pipe message: %s [b'\\x86]\\xbdW\\x01u\\x1e\\xb5\\xdb\\xc1\\xbe\\xd6\\x1f}\\x08us!\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00']\n",
      "Bad pipe message: %s [b\":\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00\"]\n",
      "Bad pipe message: %s [b'\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00']\n",
      "Bad pipe message: %s [b'\\x10\\xc0']\n",
      "Bad pipe message: %s [b'\\x15\\xc0\\x0b\\xc0\\x01']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.error.Timeout'>\n",
      "Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "## Introduction\n",
      "Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. \n",
      "\n",
      "## Our Team\n",
      "Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem.\n",
      "\n",
      "## The Problem\n",
      "The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use.\n",
      "\n",
      "But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines.\n",
      "\n",
      "## Our Solution\n",
      "And that's exactly our, what Juan Holloway is doing. Let me show you. To make a demo, we convert some results from previous hackathons into our Wikipedia.\n",
      "-------------------\n",
      "0\n",
      " Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Juan is responsible for knowing everything in your company, but first things first, our\n ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m (seg_index)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m (transcript[\u001b[39m\"\u001b[39m\u001b[39msegments\u001b[39m\u001b[39m\"\u001b[39m][seg_index][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m res,seg_index \u001b[39m=\u001b[39m add_timestamps(formatted_result, transcript,seg_index)\n\u001b[1;32m     21\u001b[0m results\u001b[39m.\u001b[39mappend(res)\n\u001b[1;32m     22\u001b[0m \u001b[39massert\u001b[39;00m seg_index \u001b[39m==\u001b[39m next_seg_index\n",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m, in \u001b[0;36madd_timestamps\u001b[0;34m(formatted_result, transcript, seg_index)\u001b[0m\n\u001b[1;32m     18\u001b[0m     pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39m#print (par[pos:])\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39massert\u001b[39;00m par[pos:]\u001b[39m.\u001b[39mstartswith(seg_text), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mseg_text\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mpar[pos:]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m seg_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     22\u001b[0m pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(seg_text)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Juan is responsible for knowing everything in your company, but first things first, our\n "
     ]
    }
   ],
   "source": [
    "seg_index = 0\n",
    "context = None\n",
    "results =[]\n",
    "while seg_index < len(transcript[\"segments\"]):\n",
    "    print (seg_index, transcript[\"segments\"][seg_index][\"text\"])\n",
    "    text, next_seg_index = next_block(transcript,seg_index) \n",
    "    print (next_seg_index-1, transcript[\"segments\"][next_seg_index-1][\"text\"])\n",
    "    prompt = get_prompt(text, context)\n",
    "    print (prompt)\n",
    "    formatted_result = process(\n",
    "            system_prompt = system_prompt,\n",
    "            user_prompt= get_prompt(text, context),\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.0\n",
    "        )\n",
    "    print (formatted_result)\n",
    "    print (\"-------------------\")\n",
    "    print (seg_index)\n",
    "    print (transcript[\"segments\"][seg_index][\"text\"])\n",
    "    res,seg_index = add_timestamps(formatted_result, transcript,seg_index)\n",
    "    results.append(res)\n",
    "    assert seg_index == next_seg_index\n",
    "    context = get_last_section(formatted_result)\n",
    "    print (context)\n",
    "\n",
    "result = \"\".join(results)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_index, next_seg_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Introduction\\nHi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem.\\n\\n## The Problem\\nThe problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use.\\n\\nBut the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines.\\n\\nAnd that's exactly our, what Juan Holloway is doing. Let me show you.\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" To make a demo, we convert some results from previous hackathons into our Wikipedia. So it's classical Wikipedia, Wikipedia that allow you edit pages, add new ones, have some structure here, and whatever, just editor for the small pages. So we converted demos of a few teams into this Wikipedia format using Whisper and some magic from GPT to make them much more readable. But what is more important, we allow them to be searchable by semantic search. So we use a multilingual semantic search, and I will show you how it works. We have some project, Ask Quran, let's try to formulate it in Russian, like a questions about God. Yeah, we have Ask Quran project. It's more delay on loading. I will skip a bit. Seems like it's quite a good match about questions about God in Russian to the English text. So let me show you how our system, our Wikipedias works with video. So when video is playing, we highlight the part of the video, or you can jump to the part of the video from the video. We could jump to the timestamp in the video, clicking to the text, or you can switch the\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, next_seg_index = next_block(transcript,16) \n",
    "next_seg_index\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hi, I'm here to introduce to you, you are our first from whole Holloway family of Enterprise Assistants. Juan is responsible for knowing everything in your company, but first things first, our team is from Peru, Moldova, Austria, and I personally live in Germany, and we all gather together to solve one simple problem. The problem is that we are really spoiled by quality of Google search. So spoiled that every time we need to find something in our corporate systems, we suffer, because they usually use the keyword search, the old style, not the modern one that Google, not modern one, semantic, that Google use. But the problem is much bigger, that a lot of knowledge is hiding in the recorded webcast, like you are looking now. Imagine that all video presentation of business processes, systems, solutions, or ever greater ideas in your company will be converted into the Wikipedia pages and will be searchable by modern search engines. And that's exactly our, what Juan Holloway is doing. Let me show you.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
